java并发编程的艺术
the art of java concurrency programming

资深java技术专家，对并发编程有非常深入的研究。

## 第1章，并发编程的挑战
并发编程的目的是为了让程序运行得更快，但并不是启动更多的线程就能让程序最大限度地并发执行。

### 上下文切换
即使是单核处理器也支持多线程执行代码，CPU给每个线程分配CPU时间片(一般几十ms)来实现此机制。
CPU通过不停地切换线程执行，让我们感觉多个线程同时执行

CPU通过时间片分配算法来循环执行任务，当迁任务执行一个时间片后会切换到下一个任务。
在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再加载这个任务的状态，任务从保存到再加载过程是一次上下文切换
会影响多线程的执行速度

1. 多线程一定快吗
当并发执行累加操作不超过百万次时，速度会比串行执行累加操作慢。
因为吸纳城有创建和上下文切换的开销

2. 测试上下文切换次数和时长
用Lmbench3可以测量上下文切换的时长
vmstat可以测量上下文切换的次数：vmstat 1，展示的CS(Context Switch)列，每1s的次数

3. 如何减少上下文切换
方法有：
+ 无锁并发编程。多线程竞争锁时，会引起上下文切换，用一些办法避免使用锁，如将数据的id按照hash算法取模分段，不同线程处理
+ CAS算法。不用加锁
+ 用最少线程。避免创建不需要的线程，比如任务很少，但创建了很多线程处理，造成大量线程处于等待状态。
+ 协程：在单线程里实现多任务的调度，在单线程里维持多个任务间的切换

4. 减少上下文切换实战
通过减少线上大量WAITING的线程，减少上下文切换次数
a. jstack pid > dump17
b. 统计线程分别处于什么状态，发现300多线程处于WAITING(onobjectmonitor)状态
grep dump11 | awk '{print $2$3$4$5}' | sort | uniq -c
c. 打开dump看WAITING(onobjectmonitor)的线程做什么。
发现这些线程基本全是JBOSS的工作线程，在await。说明JBOSS线程池里线程接收到的任务太少，大量线程都闲着。
d. 减少JBOSS的工作线程数。
e. 重启JBOSS，再dump并统计WAITING(onobjectmonitor)，WAITING的线程发现减少了，下同上下文切换次数少了。
因为每一次从WAITING到RUNNABLE都会进行一次上下文的切换。

### 死锁
出现死锁，业务可感知，因为不能提供服务了，只能dump线程看到底哪个线程出问题。

避免死锁方法：
+ 避免一个线程同时获取多个锁
+ 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源。
+ 尝试用定时锁，tryLock(timeout)
+ 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则出现解释失败情况

### 资源限制的挑战
指并发编程时，程序执行速度受限于计算机硬件资源或软件资源。并发编程时，要考虑这些资源的限制。
硬件资源限制有：带宽的上传/下载速度、硬盘读写速度和CPU的处理速度。
软件资源限制：数据库的连接数和socket连接数等

并发编程中，将代码执行速度加快的原则是，将代码中串行执行的部分变成并发执行。
但若由于资源限制，并发后仍在串行执行，可能会更慢，因为增加了上下文切换和资源调度的时间

对于硬件资源限制，可以考虑用集群并行执行程序。单机资源有限，那么就让程序在多机上运行。通过取模
对于软件资源限制，考虑用资源池将资源复用。

根据不同的资源限制调整程序的并发。
如，下载文件程序依赖两个资源——带宽和硬盘读写速度。有数据库操作时，设计数据库连接数，若sql语句执行非常快，而线程的数量比
数据库连接数大很多，则某个线程会被阻塞，等待数据库连接。

## 第2章，java并发机制的底层实现原理
java代码在编译后变成java字节码，字节码被类加载器加载到jvm中，jvm执行字节码，最终转化为汇编指令在cpu上执行。

### volatile的应用
volatile是轻量级的synchronized，在多处理器开发中保证了共享变量的"可见性"。不会引起上下文切换和调度。
可见性指当一个线程修改一个共享变量时，另一个线程能读到这个修改的值。

1. voltile的定义与实现原理
java语言规范对volatile定义：java编程语言允许线程访问共享变量，为了确保共享变量能被准确和一致地更新，线程应该确保通过
排他锁单独获取这个变量。

若一个字段声明为volatile，java线程内存模型确保所有线程看到这个变量值时一致的。

CPU的术语定义：
术语  英文  术语描述
内存屏障  memory barriers  一组处理器指令，用于实现对内存操作的顺序限制
缓冲行  cache line   CPU高速缓存中可以分配的最小存储单元。
                    处理器填写缓存行时会加载整个缓存行，现代CPU需要执行几百次CPU指令
原子操作  atomic operations  不可中断的一个或一系列操作
缓存行填充  cache line fill  当处理器识别到，从内存中读取操作是可缓存的，
                           处理器读取整个高速缓存行到适当的缓存(L1、L2、L3或所有)
(读)缓存命中  cache hit  若进行高速缓存行填充操作的内存位置仍然是下次处理器访问的地址时，
                    处理器从缓存行中读取操作数，而不是从内存读取
写命中  write hit  当处理器将操作数写回到一个内存缓存的区域时，首先检查这个缓存的内存地址是否在缓存行中，若存在则处理器
									 将这个操作数写回到缓存，而不是写回到内存，称为写命中
写缺失  write misses the cache  一个有效的缓存行被写入到不存在的内存区域

volatile如何保证可见性？
在X86处理器下，用工具获取JIT编译器生成的汇编指令
volatile Singleton instance = new Singleton();
汇编：
0x01a3deld: movb $0x0,0x1104800(%esi);
0x01a3de24: lock addl $0x0,(%esi);
有volatile修饰的共享变量，进行写操作时会多出第二行汇编代码。
lock前缀的指令在多核处理器下会引发：
+ 将当前处理器缓存行的数据写回到系统内存
+ 这个写回操作会使其他CPU中缓存了该内存地址的数据无效

为了提高处理速度，处理器不直接和内存进行通信，而是先将系统内存的数据读到内部缓存(L1,L2或其他)后在进行操作。
若对声明volatile进行写，jvm会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。在多处理器下，
为保证各个处理器的缓存是一致的，会实现缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了。
当发现自己缓存行对应的内存地址被修改，会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作时，会重新从
系统内存中把数据读到处理器缓存里。

volatile的两条实现原则：
+ Lock前缀指令会引起处理器缓存写回到内存。缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域数据。
+ 一个处理器的缓存写回到内存会导致其他处理器的缓存无效。Interl64处理器使用MESI(修改、独占、共享、无效)控制协议去维护
内部缓存和其他处理器缓存的一致性。

2. volatile的使用优化
在jdk7中LinkedTransferQueue，用volatile变量时，用一种追加字节的方式来优化队列出队和出队的性能。
将共享变量追加到64字节。很多处理器的L1、L2或L3缓存的高速缓存行是64个字节宽，不支持部分填充缓存行。意味着，若队列的头和
尾节点不足64字节，处理器会将他们读到同一个高速缓存行中，当多处理器读写头尾时，可能将整个缓存行锁定，影响效率。

用volatile变量时，都应该追加到64字节？两种场景不应该：
+ 缓存行非64字节宽的处理器
+ 共享变量不会被频繁写。因为追加字节的方式需要处理器读取更多的字节到高速缓冲区，本身就带来一定的性能消耗。


### synchronized的实现原理与应用
java中每个对象都可作为锁
当一个线程试图访问同步代码时，必须先得到锁，退出或抛出异常时必须释放锁。

synchronized在JVM里的实现原理，monitorenter指令在编译后插入到同步代码块的开始位置，monitorexit是插入到方法结束处和异常处，
jvm保证每个monitorenter必须有对应的monitorexit配对。
任何对象都有一个monitor与之关联，当且一个monitor被持有后，他将处于锁定状态。
线程执行到monitorenter指令时，将会尝试获取对象所对应的monitor的所有权，即尝试获得对象的锁

1. java对象头
synchronized用的锁是存在java对象头里的。
若对象是数组类型，则jvm用3个字宽(word)存储对象头，若对象是费数组类型，用2字宽存储对象头。32位jvm中1字宽等于4字节

java对象头的长度：
长度  内容  说明
32/64bit  Mark Word  存储，对象的hashCode或锁信息等
32/64bit  Class Metadata Address  存储，到对象类型数据的指针
32/32bit  Array length  数组的长度(若当前对象是数组)

Mark Word中默认存储对象的HashCode、分代年龄和锁标记位。

32位jvm的Mark Word默认存储结构：
锁状态  25bit  4bit  1bit是否是偏向锁  2bit锁标志位
无锁状态  对象的hashCode  对象分代年龄  0  01

运行期间，Mark Word会随着锁标志位的变化而变化，可能有4种：
锁状态  25bit     4bit  1bit是否是偏向锁  2bit锁标志位
     23bit|2bit
轻量级锁        指向栈中锁记录的指针           00
重量级锁        指向互斥量(重量级锁)的指针      10
GC标记         空                          11
偏向锁 线程ID|Epoch|对象分代年龄|1        |   01

64位jvm，Mark Word是64bit：
锁状态  25bit  31bit    1bit(cms_free)   4bit分代年龄   1bit偏向锁   2bit锁标志位
无锁   unused hashCode                                    0         01
偏向锁  ThreadID(54bit) Epoch(2bit)                       1         01


2. 锁的升级与对比
jdk6为减少获得锁和释放锁带来的性能消耗，引入偏向锁和轻量级锁，一共有4种状态，从低到高：无锁状态、偏向锁状态、
轻量级锁状态和重量级锁状态。
锁能升级却不能降级，目的是为了提高获得锁和释放锁的效率

a. 偏向锁
hotSpot作为经研究发现，多数情况，锁不仅不存在多线程竞争，而且总是由统一线程多次获得，为了让线程获得锁的代价更低而引入
偏向锁。
当一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程ID，以后该线程在进入和退出同步块时不需要进行
CAS操作来加锁和解锁，秩序测试一下对象头的Mar Word中是否存储指向当前线程的偏向锁。
若测试成功，表示线程已经获得了锁，
若测试失败，需要再测试一下Mark Word中偏向锁的标识是否为1(表示当前是偏向锁)：
  若没有设置，使用CAS竞争锁
	若设置了，则尝试用CAS将对象头的偏向锁指向当前线程。

1)偏向锁的撤销
偏向锁使用了一种等到竞争出现才释放锁的机制，所以当其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。
偏向锁的撤销，要等待全局安全点(此时没有正在执行的字节码)。会先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着，
若线程不处于活动状态，则将对象头设置成无锁状态
若线程活着，拥有偏向锁的栈会被执行，遍历偏向对象的锁记录，栈中的锁记录和对象头的Mark Word要么重新偏向于其他线程，要么恢复
到无锁或标记对象不适合作为偏向锁，最后唤醒暂停的线程。

线程要进入同步块了，先检查对象头中是否存储了自己的id
没有则cas替换markword
  成功，将对象头markword中的线程id指向自己，这时markword是1|01，执行同步体代码
	失败，撤销偏向锁，会导致暂停拥有锁线程执行，将markword中的线程id为空，markword是1|01，继续让已有锁线程执行

2)关闭偏向锁
jdk7默认启动，但在应用程序启动几秒后才激活，如有必要可让jvm关闭延迟：-XX:BiasedLockingStartupDelay=0
若确定应用程序中所有锁通常处于竞争状态，可用参数关闭偏向锁：-XX:-UseBiasedLocking=false，则程序默认进入轻量级锁状态

b. 轻量级锁
1)轻量级加锁
线程在执行同步块之前，jvm会在当前线程的栈帧中创建，用于存储锁记录的空间，并将向对象头的Markword复制到所锁记录中，称为
Displaced Mark Word。然后线程尝试用CAS将对象头的markword替换为指向锁记录的指针。
若成功，当前线程获得锁
若失败，表示其他线程竞争锁，当前线程便尝试用自旋来获取锁

2)轻量级锁解锁
用原子的CAS操作将Displaced Mark Word替换回到对象头
若成功，表示没有竞争发生
若失败，表示当前锁存在竞争，锁会膨胀成重量级锁。

因为自旋消耗CPU，为避免无用自旋(比如获得锁的被阻塞住了)，一旦锁省纪委重量级锁，就不在恢复到轻量级锁状态。当锁处于此状态下，
其他线程试图获取锁时，都会被阻塞，当持有锁的线程释放锁后会唤醒这些线程，进行下一轮争夺锁

先获取锁的线程，会将对象的markword改成指向栈的指针，后来的线程会cas失败导致自旋获取锁，当再次尝试失败时，锁膨胀，将
对象markword改成指向重量级锁的指针。而已持有线程想要释放时cas准备替换对象的markwod时发现是重量级锁了，则释放锁并唤醒
等待的线程。被唤醒的线程重新争夺锁访问同步块

c. 锁的优缺点对比
锁  优点  缺点  适用场景
偏向锁  加锁和解锁不需要额外的消耗，和执行非同步方法相比仅存在纳秒级差距  若线程存间存在锁竞争，会带来额外的锁撤销的消耗
  适用于只有一个线程访问同步块的场景
轻量级锁  竞争的线程不会阻塞，提高了程序的响应速度  若始终得不到锁竞争的线程，使用自旋会消耗CPU  追求响应时间，同步块执行
  速度非常快
重量级锁  线程竞争不使用自旋，不会消耗CPU  线程阻塞，响应时间缓慢  追求吞吐量，同步块执行速度较长


### 原子操作的实现原理
原子(atomic)本意是"不能被进一步分割的最小粒子"，原子操作(atomic operation)意为"不可被中断的一个或一系列操作"。

1. 术语定义
CPU术语定义：
术语名称  英文  解释
缓存行  cache line  缓存的最小操作单位
比较并交换  Compare and Swap  两个数值，一个旧值(期望操作前的值)，一个新值，在操作期间先比较旧值有无变化，若没有才交换
  新值，发生变化则不交换
CPU流水线  CPU pipeline  工作方式像产线上的装配流水线，在CPU中由5~6个不同功能的电路单元组成一条指令处理流水线，然后将
  一条X86指令分成5~6步后再由这些电路单元分别执行，就能实现在一个CPU时钟周期完成一条指令，提高CPU的运算速度
内存顺序冲突  Memory order violation  一般由假共享引起的，假共享指多个CPU同时修改同一个缓存行的不同部分而引起的其中
  一个CPU的操作无效，当出现这个内存顺序冲突时，CPU必须清空流水线

2. 处理器如何实现原子操作
32位IA-32处理器用基于对缓存加锁或总线加锁的方式来实现多处理器之间的原子操作。
处理器会自动保证基本的内存操作的原子性，保证从系统内存中读取或写入一个字节是原子的，即当一个处理器读取一个字节时，其他
处理器不能访问这个字节的内存地址。

针对复杂内存操作(如跨总线宽度、跨多个缓存行和跨页表的访问)，处理器提供总线锁定和缓存锁定机制保证原子性：
1)用总线锁保证原子性
要保证读改写共享变量的操作时原子，必须保证cpu1读改写共享变量时，cpu2不能操作缓存了改共享变量内存地址的缓存。
总线锁就是使用处理器提供的一个Lock#信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞，那么此处理器可以独占
共享内存。

2)使用缓存锁保证原子性
同一时刻，只需保证对某个内存地址操作原子性即可，而总线锁定把CPU和内存之间的通信锁住了，使得锁定期间，其他cpu不能操作其他
内存地址的数据，开销较大。

目前处理器可以用"缓存锁定"方式实现复杂的原子性。
缓存锁定指内存区域若被缓存在处理器的缓存行中，并且在Lock操作期间被锁定，那么当他执行锁操作写回到内存时，处理器不在总线上
声言Lock#信号，而是修改内部的内存地址，并允许它的缓存一致性机制来保证操作的原子性，因为缓存一致性机制会阻塞同时修改由两个
以上cpu缓存的内存区域数据，当其他处理器写回已被锁定的缓存行的数据时，会使缓存行无效。

两种情况处理器不会用缓存锁定：
+ 当操作的数据不能被缓存在处理器内部，或操作的数据跨多个缓存行(cache line)，则处理器用总线锁定
+ 有些处理器不支持缓存锁定。

针对以上两种场景，通过Intel处理器提供的很多Lock前缀的指令来实现，如测试和修改指令:BTS、BTR、BTC;交换指令XADD、CMPXCHG,
及其他逻辑指令(如ADD、OR)等，被这些指令操作的内存区域会加锁，导致其他处理器不能同时访问他

3. java如何实现原子操作
1)用循环CAS实现原子操作
jvm的CAS操作时利用了处理器的CMPXCHG指令实现。

2)CAS实现原子操作的三大问题
a. ABA问题。若一个值原来是A，变成B，又变成A，那么用CAS进行检查时会发现它的值没有发生变化，但实际上却变化了。
解决思路是用版本号。每次变量更新时都+1，AtomicStampedReference可用。
b. 循环时间长开销大。
自旋CAS若长时间不成功，则给CPU带来非常大的执行开销。

若jvm能支持处理器提供的pause指令，那么效率会有一定提升。
pause指令有两个作用：
+ 可以延迟流水线执行执行指令(de-pipeline)，使CPU不会消耗过多的执行资源。
+ 可以避免在退出循环时因内存顺序冲突而引起CPU流水线被清空(CPU Pipeline Flush)，从而提高CPU的执行效率

c. 只能保证一个共享变量的原子操作。
jdk5可用AtomicReference解决。

3)使用锁机制实现原子操作
锁机制保证了只有获得锁的线程才能操作锁定的内存区域。
除了偏向锁，jvm实现锁的方式都用了循环CAS(获取和释放时用CAS)


## 第3章，内存模型
### java内存模型的基础
1. 并发编程模型的两个关键问题
线程之间如何通信及线程之间如何同步(这里线程指并发执行的活动实体)。
通信是指线程之间以何种机制来交换信息。

在命令式编程中，线程之间的通信机制有两种：
+ 共享内存
线程之间共享程序的公共状态，通过写-读内存中的公共状态进行隐式通信。
+ 消息传递
线程之间必须通过发送消息显示进行通信

同步是指程序中用于控制不同线程间操作发生相对顺序的机制。
在共享内存并发模型里，同步是显示进行的。必须显示指定某个方法或某段代码需要在线程之间互斥执行。
在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的

java的并发采用的是共享内存模型，java线程之间的通信总是隐式进行。

2. java内存模型的抽象结构
java中，所有实例域、静态域和数组元素都存在堆内存，在线程之间共享。
局部变量(Local Varaibles)，方法定义参数(Formal Method Parameters)和异常处理参数(Exception Handler Parameter)
不会在线程之间共享，不会有内存可见性问题，也不受内存模型的影响

java线程之间的通信由java内存模型(jmm)控制，jmm决定一个线程对共享变量的写入何时对另一个线程可见。
抽象角度，jmm定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存(Main Memory)中，每个线程都有一个私有的本地
内存(Local Memory)，本地内存中存储了该线程以读/写共享变量的副本。

本地内存是jmm的一个抽象概念，并不真实存在，涵盖了缓存、写缓冲区、寄存器以及其他的硬件和编译器优化

jmm通过控制主内存与每个线程的本地内存之间的交互，来为java程序提供内存可见性保证。

3. 从源代码到指令序列的重排序
执行程序时，为提高性能，编译器和处理器常对指令做重排序，3种类型：
1)编译器优化的重排序。
编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序
2)指令级并行的重排序
现代处理器采用指令级并行技术(Instruction-Level Parallelism,ILP)来将多条指令重叠执行。
若不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序
3)内存系统的重排序。
由于处理器使用缓存和读/写缓冲区，使得加载和存储操作看上去可能是在乱序执行

源代码-->编译器优化重排序-->指令级重排序-->内存系统重排序-->最终执行的指令序列

这些重排序可能导致多线程程序出现内存可见性问题。
对于编译器，jmm的编译器重排序规则会禁止特定类型的编译器重排序。对于处理器重排序，jmm的处理器重排序规则会要求java编译器
在生成指令序列时，插入特定类型的内存屏障(Memory Barriers)指令，来禁止特定类型的处理器重排序。

jmm属于语言级的内存模型，确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序
员提供一致的内存可见性保证

4. 并发编程模型的分类
现代处理器使用写缓冲区临时保存向内存写入的数据。
写缓冲区可以保证指令流水线持续运行，可以避免由于处理器停下来等待向内存写入数据而产生的延迟。同时通过以批处理的方式刷新写
缓冲区，以及合并写缓冲区中对统一内存地址的多次写，减少对内存总线的占用。

但每个处理器上的写缓冲区，仅仅对它所在的处理器可见。此特性会对内存操作的执行顺序产生重要影响：处理器对内存的读/写操作的执行
顺序，不一定与内存实际发生的读/写操作顺序一致！

例如：a=1;x=b，在线程A内执行，A1:先写入a=1到写缓冲区，A2:然后读x=b从内存中，再刷新a=1到内存中。
虽然处理器A执行内存操作的顺序为A1->A2，但内存操作实际发生的顺序是A2->A1，处理器A的内存操作顺序被重排序了。
关键是，由于写缓冲区仅对自己的处理器可见，会导致处理器执行内存操作的顺序可能会与内存实际的操作执行顺序不一致。

由于现代处理器都会使用写缓冲区，因此，现代处理器都会允许对写-读操作进行重排序
常见的处理器都允许Store-Load重排序；都不允许对存在数据依赖的操作做重排序。

为了保证内存可见性，java编译器在生成指令序列的适当位置插入内存屏障指令来禁止特定类型的处理器重排序。
jmm把内存屏障指令分为4类：
屏障类型  指令示例  说明
LoadLoad Barriers  Load1;LoadLoad;Load2  确保Load1数据的装载先于Load2及所有后续装载指令的装载
StoreStore Barriers  Store1;StoreStore;Store2  确保Store1数据对其他处理器可见(刷新到内存)先于Store2及所有后续
  存储指令的存储
LoadStore Barriers  Load1;LoadStore;Store2  确保Load1数据装载先于Store2及所有后续的存储指令刷新到内存
StoreLoad Barriers  Store1;StoreLoad;Load2  确保Store1数据对其他处理器变得可见(指刷新到内存)	先于Load2及所有后续
  装载指令的装载。会使该屏障之前的所有内存访问指令(存储和装载指令)完成后，才执行该屏障之后的内存访问指令
  是一个"全能型"的屏障，同时具有其他3个屏障的效果。现代多处理器大多支持。开销昂贵，因为当前处理器通常要把写缓冲区中的数据
	全部刷新到内存中(Buffer Fully Flush)

5. happens-before简介
JSR-133内存模型，使用happens-before的概念来阐述操作之间的内存可见性。
jmm中，若一个操作执行的结果对另一个操作可见，那么这俩操作之间必须要存在happens-before关系。

与程序员密切相关的happens-before规则：
+ 程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作
+ 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁
+ volatile变量规则：对一个volatile域的写，happens-before于任何后续对这个volatile域的读
+ 传递性：若A happens-before B，且B happens-before C，那么A happens-before C

happens-before仅仅要求前一个操作(执行的结果)对后一个操作可见，且前一个操作按顺序排在第二个操作之前(the first is
visible to and ordered before the second)。

一个happens-before规则对应于一个或多个编译器和处理器重排序规则。

### 重排序
指编译器和处理器为优化程序性能而对指令序列进行重新排序的一种手段

1. 数据依赖性

若两个操作访问同一个变量，且这两个操作中有一个为写操作，此时两者之间存在数据依赖性

数据依赖类型表：
名称  代码
写后读  a=1;b=a;
写后写  a=1;a=2;
读后写  a=b;b=1;

上述3种情况，只要重排序两个操作的执行顺序，程序的执行结果就会被改变

编译器和处理器在重排序时，会遵守数据依赖性，不会改变存在数据依赖关系的两个操作的执行顺序

这里仅对单个处理中执行的指令序列和单个线程中执行的操作而言，不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑

2. as-if-serial语义
不管怎么重排序(编译器和处理器为提高并行度)，(单线程)程序的执行结果不能被改变。
编译器、runtime和处理器都必须遵守，不会对存在数据依赖关系的操作进行重排序
但，若操作之间不存在数据依赖关系，则这些操作可能被重排序

```
double pi = 3.14;  // A
double r = 1.0;  // B
double area = pi * r * r;  // C
```
A和B之间无数据依赖，C依赖于A，C依赖于B
A和B可能重排序

as-if-serial语义使单线程程序员无需担心重排序会干扰他们，也无需担心内存可见性问题

3. 程序顺序规则
根据happens-before的程序顺序规则，上面代码存在3个happens-before关系
1)A happens-before B
2)B happens-before C
3)A happens-before C (依据传递性推导出来)

这里的A happens-before B，但实际执行时B却可以排在A之前执行。jmm并不要求A一定要在B之前执行，仅仅要求前一个操作(执行
的结果)对后一个操作可见，且前一个操作按顺序排在第二个操作之前。
由于这里操作A的执行结果不需要对操作B可见，而且重排序操作A和操作B后的执行结果，与操作A和操作B按happens-before顺序执行
的结果一致。这时，jmm会认为这种重排序并不非法(not illegal)，jmm允许这种重排序。

计算机中，软硬件技术有一个共同的目标：在不改变程序执行结果的前提下，尽可能提高并行度。
jmm同样遵从这一目标

4. 重排序对多线程的影响
```java
class ReorderExample{
  int a = 0;
	boolean flag = false;
	public void writer(){  // 线程A
	  a = 1;  // 1
		flag = true;  // 2
	}

	public void reader(){  // 线程B
	  if(flag){  // 3
		  int i = a * a;  // 4
		}
	}
}
```
B执行4时，能否看到线程A在操作1对共享变量a的写入呢？
不一定
由于操作1和操作2没有数据依赖关系，编译器和处理器可以对这俩操作重排序。

同样，操作3和4也没有数据依赖关系，编译器和处理器也可以对其重排序
因为：
3和4存在控制依赖关系。
当代码中存在控制依赖性时，会影响指令序列执行的并行度。为此，编译器和处理器采用猜测(Speculation)执行来克服控制相关性
对并行度的影响。以
处理器的猜测执行为例，线程B的处理器可以提前读取并计算a*a，把结果临时保存到一个重排序缓冲(Reorder Buffer, ROB)的硬件
缓存中。当操作3条件判断为真时，就把该计算结果写入变量i中
可见，猜测执行，实质上对操作3和4做了重排序。破坏了多线程程序的语义

单线程内，对存在控制以来的操作重排序，不会改变执行结果(也是as-if-serial语义允许对存在控制依赖的操作做重排序的原因)；
但多线程内，对存在控制依赖的重排序，可能会改变程序的执行结果

### 顺序一致性
顺序一致性内存模型是一个理论参考模型，在设计时，处理器的内存模型和编程语言的内存模型都会以顺序一致性内存模型作为参照

1. 数据竞争与顺序一致性
当程序未正确同步时，可能会存在数据竞争。

java内存模型规范对数据竞争的定义：
在一个线程中写一个变量，在另一个线程读同一个变量，而且写和读没有通过同步来排序

当代码中包含数据竞争时，程序的执行违反直觉。若能正确同步，则程序是一个没有数据竞争的程序

jmm对正确同步的多线程程序的内存一致性做如下保证：
若程序是正确同步的，程序的执行将具有顺序一致性(Sequentially Consistent)——即程序的执行结果与该程序在顺序一致性内存模型
中的执行结果相同。
注：这里的同步指广义上的同步，包括(synchronized、volatile和final)的正确使用

2. 顺序一致性内存模型
理想化的理论参考模型，为程序员提供极强的内存可见性保证。两大特性：
1)一个线程中的所有操作必须按照程序的顺序来执行
2)(不管程序是否同步)所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有
线程可见。  --每个线程看到整体都是一致的
之所以能得到这个保证，是因为顺序一致性内存模型中的每个操作必须立即对任意线程可见

在顺序一致性模型中，所有操作之间具有全序关系

但，jmm中没有这个保证。未同步程序在jmm中不但整体的执行顺序是无序的，且所有线程看到的操作执行顺序也可能不一致。
例如，在当前线程把写过的数据缓存在本地内存中，在没有刷新到主内存之前，这个写操作仅对当前线程可见；其他线程看不大到，会认为
没有执行写操作。只有当其把本地内存中写过的数据刷新到主内存之后，这个写操作才能对其他线程可见。这时，当前线程和其他线程看到
的操作执行顺序将不一致。

3. 同步程序的顺序一致性效果
将ReorderExample中方法用synchronized

A执行writer后，B线程执行reader。这是一个正确同步的多线程程序。根据jmm规范，其执行的结果将与该程序在顺序一致模型中的执行
结果相同。
顺序一致性模型中，所有操作完全按程序的顺序串行执行。
在jmm中，临界区内的代码可以重排序(但jmm不允许临界区内的代码"逸出"到临界区外，那样会破坏监视器的语义)。jmm会在退出临界区
和进入临界区做一些特别处理，使得线程在这两个时间点具有与顺序一致性模型相同的内存视图。
这种临界区内重排序既提高了执行效率，又没有改变程序的执行结果。

jmm在具体实现上的基本方针：在不改变(正确同步的)程序执行结果的前提下，尽可能地为编译器和处理器的优化打开方便之门

4. 未同步程序的执行特性
jmm只提供最小安全性：线程执行时读取到的值，要么是之前某个线程写入的值，要么是默认值(0、Null、False)，jmm保证线程读操作
读取到的值不会无中生有的冒出来。
为实现最小安全性，jvm在堆上分配对象时，先对内存空间进行清零，然后才会在上面分配对象(jvm内部会同步这俩操作)。因此，在已清零
的内存空间(Pre-zeroed Memory)分配对象时，域的默认初始化已经完成了。

jmm不保证，为同步程序的执行结果与该程序在顺序一致性模型中的执行结果一致。
因为，若想要保证执行结果一致，jmm需要禁止大量的处理器和编译器的优化，对程序的执行会产生很大影响。

未同步程序在顺序一致性模型或jmm中执行时，整体是无序的，其结果无法预知。

未同步程序在两个模型中的执行特性差异：
1)顺序一致性模型保证单线程内的操作会按程序的顺序执行，而jmm不保证单线程内的操作会按程序的顺序执行。
2)顺序一致性模型保证了所有线程只能看到一致的操作执行顺序，而jmm不保证所有线程能看到一致的操作执行顺序。
3)jmm不保证对64位的long和double型变量的写操作具有原子性，而顺序一致性模型保证对所有的内存读/写都具有原子性
这与处理器总线的工作机制密切相关。

计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都通过一系列步骤完成，称为总线事务(Bus
Transaction)。每个事务会读/写内存中的一个或多个物理上连续的字。
包括：
+ 读事务(Read Transaction)。从内存传送数据到处理器
+ 写事务(Write Transaction)。从处理器传送数据到内存

总线会同步试图并发使用总线的事务。其工作机制可以把所有处理器对内存的访问以串行化的方式执行。保证了单个总线事务之中的
内存读/写操作具有原子性
在一些32位的处理器上，若对64位数据写操作具有原子，可能开销大。java语言规范鼓励但不要求jvm对64位的long和double变量
的写操作具有原子性。
从JSR-133内存模型开始(jdk5)，仅仅只允许把一个64位long/double型变量的写操作拆分为两个32位的写操作执行，任意的读操作
都必须具有原子性(即任意读操作必须要在单个读事务中执行)

### volatile的内存语义
1. volatile的特性
一个volatile变量的单个读/写操作，与一个普通变量的读/写操作都使用同一个锁来同步，他们之间的执行效果相同。
锁的happens-before规则保证释放锁和获取锁的两个线程之间的内存可见性，意味着对一个volatile变量的读，总是能看到(任意
线程)对这个volatile变量最后的写入
锁的语义决定了临界区代码的执行具有原子性。即使64位的long和double型变量，只要他是volatile变量，对该变量的读/写就具有
原子性。

volatile变量自身具有特性：
+ 可见性。对一个volatile变量的读，总是能看到(任意线程)对这个volatile变量最后的写入
+ 原子性：对任意单个volatile变量的读/写具有原子性，但类似++这种复合操作不具有原子性

2. volatile写-读建立的happens-before关系
从JSR-133开始，volatile变量的写-读可以实现线程间的通信

从内存语义的角度，volatile的写-读与锁的释放-获取具有相同的内存效果：
+ volatile写和锁的释放具有相同的内存语义
+ volatile读与锁的获取具有相同的内存语义

将ReorderExample中的变量flag改成volatile
假设线程A执行writer后，线程B执行reader。
根据happens-before规则，这个过程建立的happens-before关系可以分为3类：
1)根据程序次序规则，1 happens-before 2，3 happens-before 4
2)根据volatile规则，2 happens-before 3 (因为先发生A，后发生B)
3)根据happens-before传递性规则，1 happens-before 4

这里A线程写一个volatile变量后，B线程读同一个volatile变量。
A线程在写volatile变量之前所有可见的共享变量，在B线程读同一个volatile变量后，将立即变得对B线程可见。

3. volatile写-读的内存语义
volatile写的内存语义：
当写一个volatile变量时，jmm会把该线程对应的本地内存中的共享变量值刷新到主内存。
volatile读的内存语义：
当读一个volatile变量时，jmm会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量

对volatile写和volatile读的内存语义总结：
+ 线程A写一个volatile变量，实质上是线程A向接下来将要读这个volatile变量的某个线程发出了(其对共享变量所做修改的)消息
+ 线程B读一个volatile变量，实质上是线程B接收了之前某个线程发出的(在写这个volatile变量之前对共享变量所做修改的)消息
+ 线程A写一个volatile变量，随后线程B读这个volatile变量，这个过程实质上是线程A通过主内存向线程B发送消息

4. volatile内存语义的实现
jmm分别限制编译器和处理器重排序。

jmm对编译器制定的volatile重排序规则表：
能否重排序            第二个操作
第一个操作    普通读/写      volatile读        volatile写
普通读/写                                       NO
volatile读      NO           NO               NO
volatile写                  NO                NO

从上表可看出：
+ 当第二个操作是volatile写时，不管第一个操作是什么，都不能重排序。确保volatile写之前的操作不会被编译器重排序到volatile
写之后
-- 第二个操作的写volatile，其前的内容都被写入主存了
+ 当第一个操作是读时，不管第二个操作是什么，都不能重排序。确保volatile读之后的操作不会被编译器重排序到volatile读之前
-- 第一个操作的读volatile，其后的内容都从主存刷新过来了
+ 当第一个操作时volatile写，第二个操作是volatile读时，不能重排序

为实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。
jmm采取保守策略：
+ 在每个volatile写操作的前面插入一个StoreStore屏障  -- 保证这个store之前的store都执行
+ 在每个volatile写操作的后面插入一个StoreLoad屏障  -- 保证这个store在后面的load之前
+ 在每个volatile读操作的后面插入一个LoadLoad屏障  -- 保证这个读在后续的load之前
+ 在每个volatile读操作的后面插入一个LoadStore屏障  -- 保证这个读在后续的Store之前

volatile写插入内存屏障后生成的指令序列示意图：
普通读
普通写
StoreStore屏障  -- 禁止上面的普通写和下面的volatile写重排序。保证在volatile写之前，其前面的所有普通写操作已经对任意
处理器可见，因为StoreStore屏障保障上面所有的普通写在volatile写之前刷新到主内存
volatile写
StoreLoad屏障  -- 防止上面的volatile写和下面可能有的volatile读/写重排序。因为编译器常常无法准确判断在一个volatile写的后面是否需要插入
一个StoreLoad屏障(比如一个volatile后立即return，这就无法判断接下来是否有volatile操作了)
为了保证正确实现volatile的内存语义，jmm可采用保守策略：在每个volatile写后面或者在每个volatile读的前面插入一个StoreLoad屏障
从整体执行效率的角度考虑，jmm最后选择在每个volatiel写的后面插入一个StoreLoad屏障。
因为volatile写-读内存语义的常见模式是，一个写线程多个读线程。
当读线程的数量大大超过写线程时，选择在volatile写之后插入StoreLoad屏障将带来可观的执行效率提升。
看出jmm在实现上的一个特点：首先确保正确性，然后再追求执行效率


volatile读插入内存屏障后生成的指令序列示意图：
volatile读
LoadLoad屏障  -- 禁止下面所有普通读操作和上面的volatile读重排序
LoadStore屏障  -- 禁止下面的所有的普通写操作和上面的volatile读重排序
普通读
普通写

上述太过保守。实际执行时，只要不改变volatile写-读的内存语义，编译器可以根据具体情况省略不必要的屏障

为了提供一种比锁更轻量级的线程之间通信的机制，JSR-133决定增强volatile的内存语义：
严格限制编译器和处理器对volatile变量与普通变量的重排序，确保volatile的写-读和锁的释放-获取具有相同的内存语义。

功能上，锁比volatile更强大；在可伸缩性和执行性能上，volatile更有优势。

### 锁的内存语义
1. 锁的释放-获取建立的happens-before关系
锁除了让临界区互斥执行外，还可以让释放锁的线程向获取同一个锁的线程发送消息。

依据程序次序规则和监视器锁规则和传递性，
线程A在释放锁之前所有可见的共享变量，在线程B获取同一个锁之后，将立刻变得对B线程可见。

2. 锁的释放和获取的内存语义
当线程释放锁时，jmm会把该线程对应的本地内存中的共享变量刷新到主内存中。
当线程获取锁时，jmm会把该线程对应的本地内存置为无效，从而使得被监视器保护的临界区代码必须从主内存中读取共享变量

可看出，锁释放-获取的内存语义与volatile写-读具有相同的内存语义

对锁释放和获取的内存语义总结：
+ 线程A释放一个锁，实质上是线程A向接下来将要获取这个锁的某个线程发出了(线程A对共享变量所做修改的)消息
+ 线程B获取一个锁，实质上是线程B接收了之前某个线程发出的(在释放这个锁之前对共享变量所做修改的)消息
+ 线程A释放锁，随后线程B获取这个锁，这个过程实质上是线程A通过主内存向线程B发送消息

3. 锁内存语义的实现
ReentrantLock lock = new ReentrantLock();
lock.lock();
a++;
lock.unlock();

AQS用一个整形的volatiel变量来维护同步状态。
公平锁：
lock最后调用tryAcquire，先用getState()读volatile变量
unlock最后调用tryRelease，最后写volatile变量
根据volatile的happens-before规则，释放锁的线程在写volatile变量之前可见的共享变量，在获取锁的线程读取同一个volatile
变量后将立即变得对获取锁的线程可见
非公平锁：
lock最后调用compareAndSetState，以原子操作方式更新state，jdk文档说，此操作具有volatile读和写的内存语义

分析CAS如何同时具有volatile读和volatiel写的内存语义：
前文知道编译器不会对volatile读与volatile读后的任意内存操作重排序；不会对volatile写与volatile写前的任意内存操作重排序。
组合这俩意味着，为了同时实现volatile读和volatile写的内存语义，编译器不能对CAS于CAS前面和后面的任意内存操作重排序

分析intel X86处理器，CAS如何同时具有volatile读和volatile写的内存语义
在openjdk的atomic_windows_x86.inline.hpp，根据处理器类型决定是否为cmpxchg指令添加lock前缀。
intel手册对lock前缀说明：
1)确保对内存的读-改-写操作原子执行。现代处理器，使用缓存锁定(Cache Locking)保证指令执行的原子性。降低lock前缀指令的执行
开销，不用像之前锁定总线
2)禁止该指令，与之前和之后的读和写指令重排序
3)把写缓冲区中的所有数据刷新到内存中

第2和3点具有的内存屏障效果，足以同时实现volatile读和写的内存语义

对ReentrantLock的分析可看出，锁释放-获取的内存语义的实现至少有下列方式：
1)利用volatile变量的写-读锁具有的内存语义
2)利用CAS锁附带的volatile读和写的内存语义

4. concurrent包的实现
由于CAS同时具有volatile读和volatile写的内存语义，因此java线程之间的通信有4中方式：
1)A线程写volatile变量，随后B线程读这个volatile变量
2)A线程写volatile变量，随后B线程用CAS更新这个变量
3)A线程用CAS更新一个volatile变量，随后B线程用CAS更新这个volatile变量
4)A线程用CAS更新一个volatile变量，随后B线程读这个volatile变量

java的CAS会使用现代处理器上提供的高效机器级别的原子指令。

分析concurrent包，发现一个通用化的实现模式
首先，声明共享变量为volatile
然后，使用CAS的原子条件更新来实现线程之间的同步
同时，配合以volatile的读/写和CAS所具有的volatile读和写的内存语义来实现线程之间的通信

### final域的内存语义
1. final域的重排序规则
编译器和处理器对final域遵守两个重排序规则：
1)在构造函数内对一个final域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序
-- 先写final，然后才能赋值引用，即使用此变量
2)初次读一个包含final域的对象的引用，与随后初次读这个final域，这两个操作之间不能重排序
-- 先读引用，在读成员

```java
class FinalExample{
  int i;  // 普通变量
	final int j;  // final 变量

	static FinalExample obj;

	FinalExample(){  // 构造函数
	  i = 1;  // 写普通域
		j = 2;  // 写final域
	}

	public static void writer(){  // 写线程A执行
	  obj = new FinalExample();
	}

	public static void reader(){  // 读线程B执行
	  FinalExample object = obj;  // 读对象引用
		int a = object.i;  // 读普通域
		int b = object.j;  // 读final域
	}
}
```
假设线程A执行writer，随后，线程B执行reader

2. 写final域的重排序规则
写final域的重排序规则禁止把final域的写，重排序到构造函数之外。
此规则的实现包含2方面：
1)jmm禁止编译器把final域的写重排序到构造函数之外
2)编译器会在final域的写之后，构造函数return之前，插入一个StoreStore屏障。此屏障禁止处理器把final域的写重排序到构造函数之外

分析writer，obj = new FinalExample();包含两个步骤：
1)构造一个FinalExample类型的对象
2)把这个对象的引用赋值给引用变量obj

写final域的重排序规则可以确保：在对象引用为任意线程可见之前，对象的final域已经被正确初始化过，而普通域不具有这个保障(可能
被重排序到构造函数外，而其他线程可能看到的是零值)。


3. 读final域的重排序规则
规则是，在一个线程中，初次读对象引用于初次读该对象包含的final域，jmm禁止处理器重排序这两个动作(规则仅针对处理器)。编译器
会在读final域操作的前面插入一个LoadLoad屏障

初次读对象引用于初次读该对象包含的final域，操作之间存在间接依赖关系。编译器遵守此关系，因此不会重排序这两个操作。

reader包含3个操作
+ 初次读引用变量obj
+ 初次读引用变量obj指向对象的普通域i
+ 初次读引用变量obj指向对象的普通域j

读final域的重排序规则可以确保：在读一个对象的final域之前，一定会先读包含这个final域的对象的引用。

4. final域为引用类型
```java
class FinalReferenceExample{
  final int[] intArray;  // final是引用类型
	static FinalReferenceExample obj;

	public FinalReferenceExample(){  // 构造函数
	  intArray = new int[1];  // 1 对final域的写入
		intArray[0] = 1;  // 2 对这个final域引用的对象的成员域的写入
	}

	public static void writerOne(){  // 写线程A
	  obj = new FinalReferenceExample();  // 3 把构造的对象的引用赋值给某个引用变量
	}

	public static void writerTwo(){  // 写线程B
	  obj.intArray[0] = 2;
	}

	public static void reader(){  // 读线程C
	  if(obj != null){  // 5
		  int temp1 = obj.intArray[0];  // 6
		}
	}
}
```
对于写final域的重排序规则对编译器和处理器增加了约束：
在构造函数内对一个final引用的对象的成员域的写入，与随后在构造函数外把这个被构造对象的引用赋值给一个引用变量，
两操作之间不能重排序  -- 先设定好内部，外部才能引用看到值

final域写入规则保证，1和3不能重排序，2和3不能重排序
jmm可以确保线程C至少能看到写线程A在构造函数中对final引用对象的成员域的写入。而线程B对数元素的写入，线程C可能看到或看不到，
因为线程B和线程C之间存在数据竞争，此时结果不可预知
若想保证C看到B的写入，需要在其之间使用同步原语(lock或volatile)来确保内存可见性

5. 为什么final引用不能从构造函数内"溢出"
上面说到，写final域的重排序规则可以确保：
在引用变量为任意线程可见之前，该引用变量指向的对象的final域已经在构造函数中被正确初始化过了。

要得到这个效果，还需要一个保证：在构造函数内部，不能让这个被构造对象的引用为其他线程所见，即对象引用不能再构造函数中"逸出"。
```java
class FinalReferenceEscapeExample{
  final int i;
	static FinalReferenceEscapeExample obj;

	public FinalReferenceEscapeExample(){
	  i = 1;  // 1 写final域
		obj = this;  // 2 this引用逸出
	}

	public staic void writer(){
	  new FinalReferenceEscapeExample();
	}

	public staic void reader(){
	  if (obj != null){  // 3
		  int temp = obj.i;  // 4
		}
	}
}
```
2使得对象还未完成构造前就被其他线程可见。即使2是构造函数最后一步，且在程序中在1后面，执行reader方法的线程仍可能无法看到final
域被初始化后的值，因为1和2之间可能被重排序。

可看出：在构造函数返回前，被构造对象的引用不能为其他线程所见，因为此时的final域可能还没有被初始化。在构造函数返回后，任意线程
都将保证能看到final域正确初始化后的值

6. final语义在处理中的实现
之前提到，
写final域的重排序规则要求编译器在final域的写之后，构造函数return之前插入一个StroeStore屏障。
读final域的重排序规则要求编译器在读final域的操作前面插入一个LoadLoad屏障

由于X86处理器不对写-写操作重排序，所以StroeStore会省略，同样也不会对存在间接依赖关系的操作做重排序，所以LoadLoad也被省略

7. JSR-133为什么要增强final的语义
旧jmm中，一个最严重的缺陷是线程可能看到final域的值会改变(未初始化之前的零值-->初始化后的值)
JSR-133增强final语义，为程序员提供初始化安全保证：只要对象是正确的构造的(被构造对象的引用在构造函数中没有"逸出")，那不需要
使用同步(lock和volatile)就可以保证任意线程都能看到这个final域在构造函数中被初始化之后的值

### happens-before
jmm最核心概念

1. JMM的设计
设计jmm时，需要考虑两个关键因素：
+ 程序员对内存模型的使用。
程序员希望内存模型易于理解、易于编程。希望基于一个强内存模型来编码
+ 编译器和处理器对内存模型的实现。
编译器和处理器希望内存模型对他们的束缚越少越好，他们就可做尽可能多的优化来提高性能。希望实现一个若内存模型

由于俩因素矛盾，所以JSR-133在设计JMM时的核心目标是找到一个好的平衡点：
+ 要为程序员能提供足够强的内存可见性保证；
+ 对编译器和处理器的限制要尽可能地放松。

JMM把happens-before要求禁止的重排序分为两类：
+ 会改变程序执行结果的重排序
jmm要求编译器和处理器必须禁止这种重排序
+ 不会改变程序执行结果的重排序
jmm对编译器和处理器不做要求(jmm允许这种重排序)

可看出两点：
+ jmm向程序员提供的happens-before规则能满足程序员的需求。
happens-before规则易懂，也提供了足够强的内存可见性保证
+ jmm对编译器和处理器的束缚已经尽可能少。
jmm遵循一个基本原则：只要不改变程序的执行结果(指单线程程序和正确同步的多线程程序)，编译器和处理器怎么优化都行

2. happens-before的定义
起源于Leslie Lamport的<Time, Clocks nad Ordering of Events in a Distributed System>
Leslie使用来定义分布式系统中事件之间的偏序关系(partial ordering)。

JSR-133使用happens-before来指定两个操作之间的执行顺序。
jmm可通过happens-before关系向程序员提供跨线程的内存可见性保证

<JSR-133:Java Memory Model and Thread Specification>对happens-before定义：
1)若一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，且第一个操作的执行顺序排在第二个操作之前
jmm对程序员的承诺：a操作的结果将对b可见，且a的执行顺序排在b之前。(注意，只是jmm对成员作出的保证)
2)两个操作之间存在happens-before关系，并不意味java平台的具体实现必须按照happens-before关系指定的顺序来执行。
若重排序后的执行结果一致，那么这种重排序并不非法
jmm对编译器和处理器重排序的约束原则。
jmm这么做的原因是：程序员对这两个操作是否真的被重排序并不关心，他们关心的是程序执行时的语义不能被改变(即执行结果不能被改变)
因此happens-before本质上和as-if-serial语义是一回事。
+ as-if-serial语义保证但线程内程序的执行结果不被改变，happens-before关系保证正确同步的多线程程序的执行结果不被改变。
+ as-if-serial语义给编写单线程程序的程序员一个幻觉，是按程序的顺序来执行的。happens-before给编写正确同步的多线程程序的
程序员创造了一个幻觉，正确同步的多线程程序是按happens-before指定的顺序执行的

as-if-serial语义和happens-before的目的，都是为了在不改变程序执行结果的前提下，尽可能地提高程序执行的并行度

3. happens-before规则
<JSR-133>定义如下happens-before规则：
1)程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作
2)监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁
3)volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读
4)传递性：若A happens-before B，且B happens-before C，那么A happens-before C
5)start()规则：若线程A执行操作ThreadB.start()，那么A的ThreadB.start()操作happens-before于线程B中任意操作
6)join()规则：若线程A执行ThreadB.join()并成功返回，那么线程B中的任意操作happens-before于线程A从ThreadB.join()
操作成功返回

对1/3/4说明：
        线程A           线程B
1:线程A修改共享变量
2:线程A写volatile变量
                       3:线程B读同一个volatile变量
											 4:线程B读共享变量
分析：
+ 1 happens-before 2和3 happens-before 4 由程序顺序规则产生。
编译器和处理器都遵守as-if-serial语义，即as-if-serial语义保证了程序顺序规则。
+ 2 happens-before 3 是由volatile规则产生
对一个volatile变量的读，总是能看到(任意线程)之前对这个volatile变量最后的写入。
+ 1 happens-before 4 是由传递性规则产生。
由volatile的内存屏障插入策略和volatile的编译器重排序规则共同保证


start()规则说明，
        线程A           线程B
1:线程A修改共享变量
2:线程A执行ThreadB.start()
                       3:线程B开始执行
											 4:线程B读共享变量
分析：
+ 1 happens-before 2由程序顺序规则产生
+ 2 happens-before 4由start()规则产生
+ 根据传递性，将有1 happens-before 4，意味着，线程A在执行ThreadB.start()之前对共享变量所做修改，接下来在线程B开始
执行后都将确保对线程B可见


join()规则说明，
        线程A              线程B
1:线程A执行ThreadB.join()  2:线程B写共享变量
                          3:线程B终止
4:线程A从ThreadB.join()操作成功返回
5:线程A读共享变量

分析：
+ 2 happens-before 4由join()规则产生
+ 4 happens-before 5由程序顺序规则产生
+ 依据传递性规则，将有2 happens-before 5，意味着，线程A执行操作ThreadB.join()并成功返回后，线程B中的任意操作都将
对线程A可见

### 双重检查锁定与延迟初始化

1. 双重检查锁定的由来
```java
class UnsafeLazyInitialization{
  private static Instance instance;

	public static Instance getInstance(){
	  if (instance == null){  // 1:线程A执行
		  instance = new Instance();  // 2:线程B执行
		}
		return instance;
	}
}
```
假设A执行1的同时，B执行2。此时，线程A可能看到instance引用的对象还没有完成初始化

可对getInstance做同步处理，实现线程安全的延迟初始化。
不过将导致性能开销。若被多个线程频繁调用，则导致执行性能下降。

由于早期jvm的synchronized性能开销巨大，人们想出技巧：双重检查锁定(Double-Checked Locking)。
参见：DoubleCheckedLocking
```java
class DoubleCheckedLocking{  // 1
  private static Instance instances;  // 2

	public static Instance getInstance(){  // 3
	  if(instance == null){  // 4:第一次检查
		  synchronized(DoubleCheckedLocking.class){  // 5:加锁
			  if (instance == null){  // 4:第二次检查
				  instance = new Instance();  // 7:问题的根源出现
				}
			}  // 8
		}  // 9
		return instance;  // 10
	}  // 11
}
```
线程执行到4时，代码读取到instance不为null时，instance引用的对象有可能还没有完成初始化

2. 问题的根源
代码7(instance = new Instance())创建了一个对象，分解为
memory = allocate();  // 1:分配对象的内存空间
ctorInstance(memory);  // 2:初始化对象
instance = memory();  // 3:设置instance指向刚分配的内存地址
2和3可能被重排序。

依据java规范，所有线程在执行java程序时必须遵守intra-thread semantics。其保证重排序不会改变单线程内的程序执行结果。
即允许哪些在单线程内，不会改变单线程执行结果的重排序。
2和3虽然被重排序，但不会违反intra-thread semantics，这个重排序在没有改变单线程程序执行结果的前提下，可以提高程序的执行性能

但是多线程时由于重排序，会导致其他线程看到刚分配完内存，但并没有初始化的对象

有两种办法实现线程安全的延迟初始化：
1)不允许2和3重排序
2)允许2和3重排序，但不允许其他线程"看到"这个重排序

3. 基于volatile的解决方案
将instance用volatile修饰

上述2和3之间的重排序，在多线程环境中将被禁止。

4. 基于类初始化的解决方案
jvm在累的初始化阶段(在Class被加载后，且被线程使用之前)，会执行类的初始化。
执行类的初始化期间，jvm会去获取一个锁。可以同步多个线程对同一个类的初始化
参见：InstanceFactory
实质是：允许2和3重排序，但不允许非构造线程"看到"这个重排序

初始化一个类，包括执行这个类的静态初始化代码和初始化在这个类中声明的静态字段。

根据java语言规范，在首次发生下列任意情况时，一个类或接口类型T将被立即初始化：
1)T是一个类，而且一个T类型的实例被创建
2)T是一个类，且T中生命的一个静态方法被调用
3)T中声明一个静态字段被赋值
4)T中声明的一个静态字段被使用，而且这个字段不是一个常量字段
5)T是一个顶级类(Top Level Class)，而且一个断言语句嵌套在T内部被执行

InstanceFactory中，首次执行getInstance方法的线程将导致InstanceHolder类被初始化(情况4)

java语言规范规定，对于每一个类或接口C，都有一个唯一的初始化锁LC与之对应。从C到LC的映射，由JVM的具体实现去自由实现。
jvm在鳄梨初始化期间会获取这个初始化锁，并且每个线程至少获取一次锁来确保这个类已经被初始化过了

java语言规范对于类或接口的初始化过程：
1)通过在Class对象上同步(即获取Class对象的初始化锁)，来控制类或接口的初始化。获取锁的线程会一直等待，直到当前线程能获取
到这个初始化锁(获取锁的线程会设置state = initializing)
2)线程A执行类的初始化，同时线程B在初始化锁对应的condition上等待。  -- 看来上锁力度很小啊仅有state状态
3)线程A设置state=initialized，然后唤醒condition中等待的所有线程
4)线程B结束类的初始化处理
根据jmm规范的锁规则，存在happens-before关系，将保证：线程A执行类的初始化时的写入操作(执行类的静态初始化和初始化类中声明
的静态字段)，线程B一定能看到
5)线程C执行类的初始化的处理
第3阶段后，类已经完成初始化。因此线程C在第5阶段的类初始化处理过程相对简单一些(前面的线程A和B的类初始化过程都经历了两次
锁获取-锁释放，而线程C的类初始化处理只经历一次锁获取-锁释放)

根据jmm模型规范的锁规则，将存在happens-before关系，保证：线程A执行类的初始化时的写入操作，线程C一定能看到

总结：
通过对比两个方案，发现基于类初始化方案的实现代码更简洁。
但基于volatile的双重检查锁的方案有一个额外优势：除了可以对静态字段实现延迟初始化，还可以对实例字段实现延迟初始化。

```
--不过可以通过静态块去触发实例化，不对，因为静态字段属于类，可以共有，而实例变量，是每次创建的，而需要每个实例变量延迟
初始化，似乎是对其内部的方法调用了，那么就衍生了双重检查锁的场景了。
正好看到kotlin，var是变量可以用lateinit，而val是常量，应该是构造函数时构造，而若没有在那时构造，那么需要进行延迟
初始化了，而kotlin正好用的就是volatile的双重检查锁。那就属于类的，仅执行一次。
```

字段延迟初始化，降低了初始化类或创建实例的开销，但增加了访问被延迟初始化的字段的开销。
多数时候，正常的初始化要优于延迟初始化。
基于volatile的延迟初始化方案：确实需要对实例字段使用线程安全的延迟初始化
基于类初始化方案：确实需要对静态字段用线程安全的延迟初始化

### java内存模型综述
1. 处理器的内存模型
顺序一致性内存模型是一个理论参考模型，jmm和处理器内存模型在设计时通常会以其为参照。
设计时，jmm和处理器模型会对其做一些放松，因为若完全按照其来实现处理器和jmm，那很多处理器和编译器优化都要被禁止，影响性能

根据堆不同类型的读/写操作组合的执行顺序的放松，处理器的内存模型划分为：
+ 放松程序中写-读操作的顺序，产生了Total Store Ordering内存模型(TS)
+ 在上面的基础上，继续放松程序中写-写操作的顺序，产生了Partial Store Order内存模型(PSO)
+ 在前两条基础上，继续放松程序中读-写和读-读操作的顺序，产生了Relaxed Memory Order内存模型(RMO)和PowerPC内存模型

注意：种类处理器对读/写操作的放松，是以两个操作之间不存在数据依赖性为前提的(因为处理器要遵守as-if-serial语义，处理器不会对
存在数据依赖性的两个内存操作做重排序)

处理器内存模型的特征表：
内存模型名称  对应的处理器  Store-Load  Store-Store  Load-Load和        可更早读取到    可更早读取到
											  重排序       重排序        Load-Store重排序    其他处理器的写  当前处理器的写
TSO      sparc-TSO X64    Y                                                       Y
PSO      sparc-PSO        Y           Y                                           Y
RMO      ia64             Y           Y          Y                                Y
PowerPC  PowerPC          Y           Y          Y                  Y             Y

从上看到，所有处理器内存模型都允许写-读重排序，原因是：他们都使用了写缓冲区。写缓冲区可能导致写-读操作重排序。
所有处理器都允许更早读到当前处理器的写，原因也是因为写缓存区。由于写缓存区进队当前处理器可见，导致当前处理器可以比其他
处理器先看到临时保存在自己写缓冲去中的写

从上到下，模型由强变弱，越是追求性能的处理器，内存模型设计得会越弱，因为他们希望内存模型对他们的束缚越少越好，他们就可以
尽可能多的优化来提高性能

由于常见的处理器内存模型比jmm要弱，java编译器在生成字节码时，
会在执行指令序列的适当位置插入不同的内存屏障来限制处理器的重排序。

为了在不同处理器平台向程序员展示一个一致的内存模型，jmm在不同的处理器中插入的内存屏障数量和种类不同，
对于上面表中，每个处理器有Y的，就插入对应的xx Barriers。
jmm屏蔽了不同处理器内存模型的差异，在不同的处理器平台之上为程序员呈现了一个一致的内存模型

2. 各内存模型之间的关系
jmm是一个语言级的内存模型，处理器内存模型是硬件级的内存模型，顺序一致性内存模型是一个理论参考模型。

可看出，常见的4种处理器内存模型比常用的3种语言内存模型都要弱，处理器内存模型和语言内存模型都比顺序一致性内存模型要弱
越是追求执行性能的语言，内存模型设计得会越弱

3. jmm的内存可见性保证
按程序类型，java程序的内存可见性保证分为3类：
+ 单线程程序。
不会出现内存可见性问题。编译器、runtime和处理器会共同确保单线程程序的执行结果与该程序在顺序一致性模型中的执行结果相同。
+ 正确同步的多线程程序。
程序的执行将具有顺序一致性。jmm关注的重点，通过限制编译器和处理器的重排序来为程序员提供内存可见性保证
+ 未同步/未正确同步的多线程程序。
jmm为他们提供了最小安全性保障：线程执行时读取到的值，要么是之前某个线程写入的值，要么是默认值(0、null、false)

4. JSR-133对旧内存模型的修补
两个：
+ 增强volatile的内存语义。
严格限制volatile变量与普通变量的重排序，使volatile的写-读和锁的释放-获取具有相同的内存语义
+ 增强final的内存语义。
增加了两个重排序规则，在保证final引用不会从构造函数内逸出的情况下，final具有了初始化安全性


## 第4章，java并发编程基础

线程作为操作系统调度的最小单元，多个线程能同时执行，将显著提升程序性能，多核环境更加明显。

### 线程简介
1. 什么是线程
现代操作系统在运行一个程序时，会为其创建一个进程。
现代操作系统调度的最小单元是线程，轻量级进程(Light Weight Process)，一个进程里可以创建多个线程，这些线程拥有各自的计数器、
堆栈和局部变量等属性，能访问共享的内存变量。
处理器在这些线程上告诉切换，让使用者感觉同时运行

一个java程序从main()方法开始执行，执行main()方法的是一个名称为main的线程。
参见：MultiThread
[4]Signal Dispatcher  // 分发处理发送给jvm信号的线程
[3]Finalizer  // 调用对象finalize方法的线程
[2]Reference Handler  // 清除Reference的线程
[1]main  // main线程，用户程序入口

2. 为什么要用多线程
多主要原因：
+ 更多的处理器核心
处理器拥有更多的核心
线程是大多数操作系统调度的基本单元，一个程序作为一个进程来运行，程序运行过程中能创建多个线程，而一个线程在一个时刻只能
运行在一个处理器核心上。
若程序使用多线程技术，将计算逻辑分配到多个处理器核心上，会显著减少程序的处理时间，并随着更多处理器核心的加入而变得更效率

+ 更快的响应时间
用多线程技术，将数据一致性不强的操作派发给其他线程处理(也可用消息队列)。好处是响应用户请求的线程能尽快处理完成，缩短了
响应时间，提升用户体验

+ 更好的编程模型
java为多线程编程提供了良好、考究并且一致的编程模型，使开发人员能更加专注于问题的解决，即为所遇到的问题建立合适的模型。
一旦开发人员建立好了模型，稍作修改总是能方便地映射到java提供的多线程编程模型上

3. 线程优先级
现代操作系统基本采用时分的形式调度运行的线程，操作系统会分出一个个时间片，线程会分配到若干时间片，当线程的时间片用完了
就会发生线程调度，并等待下次分配。
参见：Priority

jstack看到，都是os_prio=31
输出看到，线程优先级没有生效，表示程序正确性不能依赖线程的优先级高低

4. 线程的状态
java线程在运行的生命周期中可能处于下表不同状态，在给定的一个时刻，线程只能处于其中的一个状态

java线程的状态：
状态名称  说明
NEW  初始状态，线程被构造，但还没有调用start()
RUNNABLE  运行状态，java线程将操作系统中的就绪和运行状态统称为"运行中"
BLOCKED  阻塞状态，表示线程阻塞于锁
WAITING  等待状态，表示线程进入等待状态，表示当前线程需要等待其他线程做出一些特定动作(通知或中断)
TIME_WAITING  超时等待，是可以在指定的时间自行返回的
TERMINATED  终止状态，表示当前线程已经执行完毕


用jstack查看线程状态
参见：ThreadState

线程在自身的生命周期中，并不是固定地处于某个状态，而是随着代码的执行在不同的状态之间进行切换
参见：threadtransform.jpg
阻塞状态时线程阻塞在进入synchronized的方法或代码块时的状态，但在java.concurrent包中Lock接口的线程状态却是等待状态，因为Lock接口
对于阻塞的实现均使用了LockSupport类中的相关方法

5. Daemon线程
是一种支持型线程，它主要被用作程序中后台调度以及支持性工作。
当一个jvm不存在非Daemon线程时，jvm将会推出。Deamon线程中的finally块并不一定会执行。
参见：Daemon
在构建Daemon线程时，不能依靠finally块中的内容来确保执行关闭或清理资源的逻辑

### 启动和终止线程
1. 构造线程
先由父线程(当前线程)构造一个线程对象(new分配空间)并初始化。
Thread.init中获取currentThread作为parent，并以此设定相关属性：daemon/priority/contextClassLoader。
将父线程的InheritableThreadLocal复制过来，分配一个线程ID。
在堆内存中等待着运行

2. 启动线程
调用start即可启动线程。
start含义：当前线程(即parent线程)同步告知jvm，只要线程规划器空闲，应理解启动调用start方法的线程
启动一个线程前，最好为这个线程设置线程名称。

3. 理解中断
可理解为线程的一个标示位属性，表示一个运行中的线程是否被其他线程进行了中断操作。通过interrupt()
线程通过检查自身是否被中断来进行响应，通过isInterrupted判断，也可以通过Thread.interrupted判断并复位。若线程已经处于终结状态，即使
该线程被中断过，isInterrupted返回false

许多抛出InterruptedException的方法(如Thread.sleep)，在抛出InterruptedException之前，jvm会现将该线程的中断标识位清除，然后抛出
InterruptedException，此时调用isInterrupted返回false

参见：InterruptedTest

4. 过期的suspend/resume/stop
不建议的主要原因有：以suspend为例，调用后，线程不会释放已占有的资源(如锁)，而是占着资源进入睡眠状态，容易引发死锁问题。
stop在终结一个线程时不会保证线程的资源正常释放，通常是没有给予线程完成资源释放工作的机会，导致程序可能不稳定

5. 安全地终止线程
中断状态是线程的一个标识位，中断操作是一个简便的线程间交互方式，最适合用来取消或停止任务。
还可用一个boolean变量控制是否需要停止任务
参见：Shutdown
能使线程在终止时有机会去清理资源。

### 线程间通信
每个运行线程，仅仅孤立运行，没有一点价值，多多线程能够配合完成工作，将会带来巨大价值

1. volatile和synchronized关键字
java支持多个线程同时访问一个对象或对象的成员，由于每个线程可以拥有这个变量的拷贝(多处理器加速缓存)，所以程序在执行中，
一个线程看到的变量并不一定是最新的

volatile可以用来修饰字段，告知程序任务对该变量的访问均需从共享内存中获取，而对他的改变必须同步刷新回共享内存，他能保证
所有线程对变量访问的可见性
但是过多地使用volatile是不必要的，因为他会降低程序的执行效率

synchronized可修饰方法或同步块，主要确保多个线程在同一时刻，只能有一个线程处于方法或同步块中，它保证了线程对变量访问的
可见性和排他性

分析synchronized实现细节，用javap
参见：Synchronized

对于同步块实现使用了monitorenter和monitorexit指令，同步方法依靠方法修饰符上的ACC_SYNCHRONIZED完成同步。
本质都是对一个对象的监视器(monitor)进行获取，这个过程是排他的。

任意一个对象都拥有自己的监视器，当这个对象由同步块或这个对象的同步方法调用时，执行方法的线程必须先获取到该对象的监视器才能
进入同步块或方法中，没有获取到监视器的线程将被阻塞在入口处，进入BLOCKED状态

--Monitor.Enter-->                监视器Monitor--成功获取锁-->对象Object--Monitor.Exit-->
                                  ↑      ↓
|-------------------------------> ↑      --Monitor.Enter失败--->↓
<--Monitor.Exit后通知，出队列<--同步队列SynchronizedQueue      <--

2. 等待/通知机制
一个线程修改了一个对象的值，另一个线程感知到了变化，然后进行相应的操作。前者是生产者，后者是消费者，这种模式隔离了"做什么"(
what)和"怎么做"(How)，在功能层面上实现了解耦，体系结构上具备了良好的伸缩性。

一种方案：
```
while(value != desire){
  Thread.sleep(1000);
}
doSomething();
```
不满足时就睡眠一会，目的是防止过快的"无效"尝试，有问题：
+ 难以确保及时性。
睡眠时，基本不消耗处理器资源，但若睡得过久，就不能及时发现条件已经变化，难以保证及时性
+ 难以降低开销。
若睡眠时间1ms，这样消费者能更加迅速地发现条件变化，但可能消耗更多的处理器资源，造成无端的浪费

java通过内置的等待/通知机制
参见：WaitNotify

用wait、notify、notifyAll注意的细节：
+ 使用wait、notify、notifyAll前需要先对调用对象加锁
+ 调用wait后，线程由RUNNING变为WAITING，并将当前线程放置到对象的等待队列
+ notify或notifyAll调用后，等待线程依旧不会从wait返回，需要调用notify或notifyAll的线程释放锁后，等待线程才有机会从wait返回
+ notify方法将等待队列中的一个等待线程从等待队列中移到同步队列中，notifyAll将所有移动，
被移动的线程状态由WAITING变成BLOCKED  --准备竞争
+ 从wait返回的前提是获取了调用对象的锁

3. 等待/通知的经典范式
等待方遵循原则：
+ 获取对象的锁
+ 若条件不满足，那么调用对象的wait，被通知后仍要检查条件
+ 条件满足则执行对应的逻辑
```
synchronized(对象){
  while(条件不满足){
	   对象.wait();
	}
	对应的处理逻辑
}
```

通知方遵循的原则：
+ 获取对象的锁
+ 改变条件
+ 通知所有等待在对象上的线程
```
synchronized(对象){
  改变条件
	对象.notifyAll();
}
```

4. 管道输入/输出流
与普通的文件输入/输出流或网络输入/输出流不同之处在于，PipedWriter/PipedReader主要用于线程之间的数据传输，
而传输的媒介为内存
参见：PipedTest

5. Thread.join()使用
A线程执行了thread.join()，含义：线程A等待thread线程终止之后才从thread.join()返回。
参见：JoinTest
每个线程运行的前提是前驱线程的终止，每个线程等待前驱线程终止后，才从join返回这里涉及了等待/通知机制
```join
synchronized void join{  // 加锁目标线程对象
  while (isAlive()) {  // 测试目标线程是否存活，条件不满足，继续等待
    wait(0);
  }
	// 条件符合，方法返回
}
```

6. ThreadLocal的使用
线程变量，以ThreadLocal对象为键、任意对象为值的存储结构。
这个结构附带在线程上，即一个线程可以根据一个ThreadLocal对象查询到绑定在这个线程上的一个值。
参见：Profiler

### 线程应用实例
1. 等待超时模式
场景：超时等待
参见：TimedWaitTeset

2. 一个简单的数据库连接池示例
参见：ConnectionPool
设定线程池大小10，通过调节客户端的线程数来模拟无法获取连接的场景
从数据看出，在资源一定的情况下(连接池中10个连接)，随着客户端线程的逐步增加，客户端出现超时无法获取连接的比率不断升高。
超时返回，是系统的一种自我保护机制。
针对昂贵资源(如数据库连接)的获取都应该加以超时限制

3. 线程池技术及其示例
线程池技术，预先创建了若干数量的线程，并且不能由用户直接对线程的创建进行控制，重复使用固定或较为固定数目的线程完成任务。
好处是，一方面，消除了频繁创建和消亡线程的系统资源开销，另一方面，面对过量任务的提交能平缓的劣化

参见：ThreadPool
线程池的本质就是，
使用了一个线程安全的工作队列连接工作者线程和客户端线程，客户端线程将任务放入工作队列返回，工作者线程则不断地从工作队列
上取工作并执行。
当工作队列为空时，所有工作者线程均等待在工作队列上，当游客户端提交一个任务后会通知任意一个工作者线程，随着大量的任务被
提交，更多的工作者线程会被唤醒。

4. 一个基于线程池技术的简单web服务器
浏览器支持多线程访问，并发请求资源。
参见：SimpleHttpServer
可看到，随着线程池中线程数量的增加，SimpleHttpServer的吞吐量不断增大，响应时间不断变小，线程池作用非常明显
但，线程池中的线程数量并不是越多越好，具体的数量需要评估每个任务的处理时间，以及当前计算机的处理器能力和数量。
使用的线程过少，无法发挥多核处理器性能；使用线程过多，将会增加系统的无故开销，相反作用


## 第5章，java中的锁
锁是用来控制多个线程访问共享资源的方式。
Lock接口，虽然缺少了隐式获取释放锁的便捷性，但拥有了锁获取与释放的可操作性、可中断的获取锁及超时获取锁等多种特性。
```java
// Lock使用方式
Lock lock = new ReentrantLock();
lock.lock();
try{
}finally{
  lock.unlock();
}
```
不要将获取的过程放在try中，因为若在获取锁(自定义锁的实现)时，发生了异常，异常抛出的同时，也导致锁无故释放
-- 是不是会释放别人获取了的锁？应该是，因为锁是同一个，大家都来用lock而只有一个可以成功，其他的要阻塞，而不能由于自己的异常给lock释放了

Lock接口提供的synchronized不具备的主要特性：
+ 尝试非阻塞地获取锁
去尝试，不行就返回false
+ 能被中断地获取锁
获取锁的过程中可被其他线程中断
+ 超时获取锁


### 队列同步器(AbstractQueuedSynchronizer)
用来构建锁或其他同步组建的基础框架，用了一个int成员变量表示同步状态，通过内置的FIFO队列完成资源获取线程的排队工作。
作者期望它能成为实现大部分同步需求的基础

同步器是实现锁的关键，在锁的实现中聚合同步器，利用同步器实现锁的语义。
二者关系：锁时面向使用者，定义了使用者与锁交互的接口，隐藏了实现细节；同步器面向的是锁的实现者，简化了锁的实现方式，屏蔽
了同步状态管理、线程的排队、等待与唤醒等底层操作。
锁和同步器很好地隔离了使用者和实现者所关注的领域

1. 队列同步器的接口与示例
同步器的设计是基于模板方法模式的。

同步器可重写的方法
+ tryAcquire
独占式获取同步状态，实现该方法需要查询当前状态并判断同步状态是否符合预期
+ tryRelease
独占式释放同步状态
+ tryAcquireShared
共享式获取同步状态，返回>=0的值表示成功，反之失败
+ tryReleaseShared
共享式释放同步状态
+ isHeldExclusively
当前同步器是否在独占模式下线程占用，一般表示是否被当前线程所独占

同步器提供的模板方法：
+ acquire
独占式获取同步状态，若当前线程获取成功，则由该方法返回，否则，将会进入同步队列等待，该方法会调用重写的tryAcquire
+ acquireInterruptibly
与acquire相似，但响应中断，抛出InterruptedException
+ tryAcquireNanos
在acquireInterruptibly基础上增加超时限制，若当前线程在超时时间内没获取到同步状态，返回false，若获取到则返回true
+ acquireShared
共享式的获取同步状态，若当前线程未获取到同步状态，将会进入同步队列等待，同一时刻可以由多个线程获取到同步状态。
+ acqireSharedInterruptibly
与acquireShared相同，响应中断
+ tryAcquireSharedNanos
在acqireSharedInterruptibly基础上增加超时限制
+ release
独占式的释放同步状态，之后，将同步队列中第一个节点包含的线程唤醒
+ releaseShared
共享式的释放同步状态
+ getQueuedThreads
获取等待在同步队列上的线程集合

独占锁，就是在同一时刻只能有一个线程获取到锁，其他获取锁的线程只能处于同步队列中等待，只有获取锁的线程释放了锁，
后继的线程才能获取锁。
参见：Mutex

2. 队列同步器的实现分析
1) 同步队列
同步器依赖内部的同步队列(一个FIFO双向队列)完成同步状态的管理，
当前线程获取同步状态失败时，同步器会将当前线程以及等待状态等信息构造成一个Node并将其加入同步队列，同时会阻塞当前线程。
当同步状态释放时，会把首节点中的线程唤醒，使其再次尝试获取同步状态。

节点的属性类型与名称及描述(注意：等待队列、同步队列)
+ waitStatus，等待状态，包含：
  + CANCELLED，1，由于在同步队列中等待的线程，在等待超时或被中断，需要从同步队列中取消等待，节点进入该状态将不会变化
	+ SIGNAL，1，后继节点的线程处于等待状态，而当前节点的线程若释放了同步状态或被取消，将会通知后继节点，
	使后继节点的线程得以运行
	+ CONDITION，-1，节点在`等待队列`中，节点线程等待在Condition上，当其他线程对Condition调用了signal后，该节点将会
	从等待队列中转移到同步队列中，加入到对同步状态的获取中
	+ PROPAGATE，-3，表示下一次共享式同步状态获取将会无条件地被传播下去
	+ INITIAL，0，初始状态
+ Node prev
前驱节点，当前节点加入同步队列时被设置(尾部添加)
+ Node next
后继节点
+ Node nextWaiter
等待队列中的后继节点。若当前节点是共享的，那这个字段将是一个SHARED常量，即节点类型(独占和共享)和等待队列中的后继节点
公用同一个字段
+ Thread thread
获取同步状态的线程

节点是构成同步队列的基础，同步器拥有首节点(head)和尾节点(tail)，获取失败的线程会成为节点加入该队列的尾部。

失败线程加入队列尾部的过程必须保证线程安全，因此同步器有一个基于CAS的设置尾节点的方法：compareAndSetTail，只有设置
成功后，当前节点才正式与之前的尾节点建立关联

同步队列遵循FIFO，首节点是获取同步状态成功的节点，首节点的线程在释放同步状态时，将会唤醒后继节点，后继节点将会在获取
同步状态成功时将自己设置为首节点。
设置首节点，是通过获取同步状态成功的线程来完成的，只有一个线程能成功获取到同步状态，因此设置头节点的方法只需要将首节点
设置为原首节点的后继节点并断开原首节点的next引用即可。

2)独占式同步状态与获取
用acquire获取同步状态，该方法对中断不敏感，即由于线程获取同步状态失败后进入同步队列中，后续对线程进行中断操作时，线程
不会从同步队列中移出。
```
public final void acquire(int arg){
  if(!tryAcquire(arg) && acquireQueued(addWaiter(Node.EXCLUSIVE),arg)){
	  selfInterrupt();
	}
}
```
主要完成了同步状态获取、节点构造、加入同步队列以及在同步队列中自旋等待的相关工作。
主要逻辑：先用tryAcquire方法，其保证线程安全的获取同步状态，若获取失败，则构造同步节点(独占式Node.EXCLUSIVE，同一
时刻只能有一个线程成功获取同步状态)并通过addWaiter将该节点加入到同步队列的尾部，
最后用acquireQueued使得该节点以"死循环"的方式获取同步状态，若获取不到则阻塞节点中的线程，而被阻塞线程的唤醒，主要
依靠前驱节点的出队或阻塞线程被中断来实现

```java
private Node addWaiter(Node node){
  Node node = new Node(Thread.currentThread(), mode);
	// 快速尝试在尾部添加
	Node pred = tail;
	if(pred != null){  // 有尾部节点
	  node.prev = pred;  // 本地设定前置，无并发
		if(compareAndSetTail(pred, node)){
		  // 设定tail为自己成功
		  pred.next = node;
			return node;
		}
	}
	enq(node);
	return node;
}

private Node enq(final Node node){
  for(;;){
	  Node t = tail;
		if(t == null){  // Must initialize，当前队列没有任何节点
		  if(compareAndSetHead(new Node())){
			  // 成功设定head为当前自己，顺便设定tail也是自己
				tail = head;
			}
		}else{  // 有尾部
		  node.prev = t;  // 先本地设定自己的前置为tail
			if(compareAndSetTail(t, node)){
			  // 成功设定tail为当前节点后
				t.next = node;
				return t;
			}
		}
	}
}
```
通过用compareAndSetTail确保节点能被线程安全添加。
enq中，同步器通过"死循环"保证节点的正确添加，其中只有通过CAS将节点设置成尾节点之后，当前线程才从方法返回，否则，当前
线程不断地尝试设置。将并发添加节点的请求通过CAS变得"串行化"了。

之后，
进入一个自旋过程，每个节点(或每个线程)都在自省地观察，当条件满足，获取到了同步状态，就可以从这个自旋过程退出，否则依旧
留在这个自旋过程中(并会阻塞节点的线程)。
```
final boolean acquireQueued(final Node node, int arg){
  boolean failed = true;
	try{
	  boolean interrupted = false;
		for(;;){
		  final Node p = node.predecessor();
			if(p == head && tryAcquire(arg)){  // 前置是head且尝试获取锁成功
			  setHead(node);  // 设定head为自己
				p.next = null;  // heap GC，将老的head的next设定null
				failed = false;
				return interrupted;
			}
			if(shouldParkAfterFailedAcquire(p, node)) && parkAndCheckInterrupt()){
			  interrupted = true;
			}
		}
	}finally{
	  if(failed){
		  cancelAcquire(node);
		}
	}
}
```
当前线程在"死循环"中尝试获取同步状态，只有前驱节点是头结点才能尝试获取同步状态。因为，
+ 头结点是成功获取同步状态的节点，而头结点的线程释放了同步状态后，将会唤醒其后继节点，后继节点的变成被唤醒后需要检查
自己的前驱节点是否是头结点。
+ 维护同步队列的FIFO原则。方法中，节点自旋获取同步状态。
由于非首节点线程前驱节点出队或被中断而从等待状态返回，随后检查自己的前驱是否是头节点，若是，则尝试获取同步状态。
节点和节点之间在循环检查过程中基本不相互通信，而是简单地判断自己的前驱是否为头结点，这样就使得节点的释放规则符合FIFO，
并也便于对过早通知的处理(过早通知指前驱节点不是头结点的线程由于中断而被唤醒)。  --过早则也只能继续等待

前驱节点为头结点且能够获取同步状态的判断条件，和，线程进入等待状态，是获取同步状态的自旋过程。


通过调用同步器的release可以释放同步状态，其在释放了同步状态后，会唤醒后继节点(进而使后继节点重新尝试获取同步状态)。
```
public final boolean release(int arg){
  if(tryRelease(arg)){
	  Node h = head;
		if( h!=null && h.waitStatus != 0){
		  unparkSuccessor(h);
		}
		return true;
	}
	return false;
}
```
会唤醒头结点的后继节点线程，unparkSuccessor使用LockSupport来唤醒处于等待状态的线程

总结：在获取同步状态时，同步器维护一个同步队列，获取状态失败的线程都被加入到线程中并在队列中进行自旋；移出队列(或停止
自旋)的条件是，前驱节点为头节点且成功获取了同步状态。
在释放同步状态时，同步器调用tryRelease释放同步状态，然后唤醒头结点的后继节点

3)共享式同步状态获取与释放
与独占式获取最主要区别在于，同一时刻能否有多个线程同时获取到同步状态。

调用同步器的acquireShared可以共享式地获取同步状态
```java
public final void acquireShared(int arg){
  if(tryAcquireShared(arg)<0){  // 尝试获取同步状态，返回大于等于0表示成功
	  doAcquireShared(arg);
	}
}

private void doAcquireShared(int arg){  // 自旋过程
  final Node node = addWaiter(Node.SHARED);
	boolean failed = true;
	try{
	  boolean interrupted = false;
		for(;;){
		  final Node p = node.predecessor();
			if(p == head){  // 当前节点的前驱为头结点，尝试获取同步状态，若返回大于等于0表示成功并退出自旋
			  int r = tryAcquireShared(arg);
				if(r >= 0){
				  setHeadAndpropagate(node, r);
					p.next = null;
					if(interrupted){
					  selfInterrupt();
					}
					failed = false;
					return;
				}
			}
			if(shouldParkAfterFailedAcquire(p, node) && parkAndCheckInterrupt()){
			  interrupted = true;
			}
		}
	}finally{
	  if(failed){
		  cancelAcquire(node);
		}
	}
}
```

释放同步状态用releaseShared
```
public final boolean releaseShared(int arg){
  if(tryReleaseShared(arg)){
	  doReleaseShared();
		return true;
	}
	return false;
}
```
释放同步状态后，将唤醒后续处于等待状态的节点。
对于能支持多个线程同时访问的并发组件(如Semaphore)，它和独占式主要区别在于tryReleaseShared必须确保同步状态(或资源数)
线程安全释放，一般通过循环和CAS保证，因为释放同步状态的操作会同时来自多个线程

4)独占式超时获取同步状态

同步器提供的acquireInterruptibly在等待获取同步状态时，若当前线程被中断，会立刻返回，抛出InterruptedException

调用doAcquireNanos在支持响应中断的基础上，增加了超时获取的特性。
主要计算出需要睡眠的时间间隔nanosTimeout，为了防止过早通知，公式为：nanosTimeout-=now(当前)-lastTime(上次唤醒时间)，
若大于0表示超时未到，需要继续睡眠nanosTimeout纳秒，反之，表示已经超时。
```java
private boolean doAcquireNanos(int arg, long nanosTimeout){
  long lastTime = System.nanoTime();
	final Node node = addWaiter(Node.EXCLUSIVE);
	boolean failed = true;
	try{
	  for(;;){
		  final Node p = node.predecessor();
			if(p == head && tryAcquire(arg)){
			  setHead(node);
				p.next = null;  // help GC
				failed = false;
				return true;
			}
			if(nanosTimeout <= 0){
			  return false;
			}
			if(shouldParkAfterFailedAcquire(p, node) && nanosTimeout > spinForTimeoutThreadhold){
			  LockSupport.parkNanos(this, nanosTimeout);
			}
			long now = System.nanoTime();
			// now减去方法刚进入的时间得到已耗费的时间，然后被原有超时时间nanosTimeout减去，得到还应该睡眠的时间
			nanosTimeout -= now - lastTime;
			lastTime = now;
			if(Thread.interrupted()){
			  throw new InterruptedException();
			}
		}
	}finally{
	  if(failed){
		  cancelAcquire(node);
		}
	}
}
```
自旋过程中，当节点的前驱为头结点时尝试获取同步状态，若获取成功则返回，与独占式类似。
在同步状态获取失败时，判断是否超时(nanosTimeout小于等于0表示超时)，若没有，重新计算nanosTimeout，然后使当前线程等待
nanosTimeout纳秒(当已到设置的超时时间，该线程会从LockSupport.parkNanos方法返回)
若nanosTimeout小于等于spinForTimeoutThreadhold(1000纳秒)时，将不会使该线程进入超时等待，而是进入快速的自旋过程。
原因是，非常短的超时等待无法做到十分精确，会让nanosTimeout的超时从整体上表现得不精确

与acquire主要区别在于未获取到同步状态时的处理逻辑：
+ acquire在未获取到同步状态时，会使当前线程一直处于等待状态
+ doAcquireNanos会使当前线程等待nanosTimeout纳秒，若当前线程在nanosTimeout纳秒内没有获取到同步状态，将会从等待逻辑中
自动返回


5)自定义同步组件——TwinsLock
目的：该工具在同一时刻，只允许至多两个线程同时访问，超过两个线程的访问将被阻塞。
+ 确定访问模式，因为同一时刻支持多个线程的访问，是共享式访问。
+ 定义资源数，在同一时刻允许至多两个线程同时访问，表明同步资源数为2，status=2，合法范围为0、1、2
+ 组合自定义同步器。
参见：TwinsLock
同步器作为一个桥梁，连接线程访问以及同步状态控制等底层技术与不同并发组件(如Lock、CountDownLatch等)的接口语义


### 重入锁
支持重进入的锁，表示该锁能支持一个线程对资源的重复加锁。
Mutex不支持，synchronized支持重入。ReentrantLock支持。

ReentrantLock支持公平锁，先对锁进行获取的请求一定先被满足。即等待时间最长的线程优先获取锁，获取锁是顺序的。
公平锁的机制往往没有非公平的效率高，但，并不是任何场景都以TPS作为唯一的指标，公平锁能减少"饥饿"发生的概率，等待越久的请求越是
能得到优先满足。

1)重现重进入
指任意线程在获取到锁后能再次获取该锁而不会被锁所阻塞，该特性的实现需要解决两个问题：
+ 线程再次获取锁。
锁需要识别获取锁的线程是否为当前占据锁的线程
+ 锁的最终释放。
线程重复n次获取锁，随后在第n次释放锁后，其他线程能够获取到该锁。要求锁进行计数。

ReentrantLock通过组合自定义同步器来实现锁的获取与释放
```java
final boolean nonfairTryAcquire(int acquires){
  final Thread current = Thread.currentThread();
	int c = getState();
	if(c == 0){  // 当前锁没有线程获取
	  if(compareAndSetState(0, acquires)){  // 尝试
		  setExclusiveOwnerThread(current);
			return true;
		}
	}else if(current == getExclusiveOwnerThread()){  // 是否自己已获取过锁
	  int nextc = c + acquires;
		if(nextc < 0){
		  throw new Error("Maximum lock count exceeded");
		}
		setState(nextc);
		return true;
	}
	return false;
}
```
方法增加了再次获取同步状态的处理：判断当前线程是否为获取锁的线程来决定获取操作是否成功，若是则将同步状态进行增加并返回true，
表示获取同步状态成功。

释放时也减去同步状态值
```java
protected final boolean tryRelease(int releases){
  int c = getState() - release;
	if(Thread.currentThread() != getExclusiveOwnerThread()){
	  throw new IllegalMonitorStateException();
	}
	boolean free = false;
	if(c == 0){
	  free = true;
		setExclusiveOwnerThread(null);
	}
	setState(c);
	return free;
}
```
若该锁获取了n次，那么前(n-1)次tryRelease返回false，只有同步状态完全释放了，才能返回true。
该方法将同步状态是否为0作为最终释放的条件，为0将占有线程设置为null，返回true，表示成功。

2)公平与非公平获取锁的区别
若一个锁时公平的，那么锁的获取顺序就应该符合请求的绝对时间顺序，FIFO
```java
protected final boolean tryAcquire(int acquires){
  final Thread current = Thread.currentThread();
	int c = getState();
	if(c == 0){
	  if(!hasQueuedPredecessors() && compareAndSetState(0, acquires)){  // 之前队列中不存在节点 且 cas成功
		  setExclusieOwnerThread(current);
			return true;
		}
	}else if(current == getExclusiveOwnerThread()){  // 重入
	  int nextc = c + acquires;
		if(nextc < 0){
		  throw new Error("Maximum lock count exceeded");
		}
		setState(nextc);
		return true;
	}
	return false;
}
```
与nonfairTryAcquire唯一不同的是判断条件多了hasQueuedPredecessors，即加入了同步队列中当前节点是否有前驱节点的判断，
若返回true，表示有线程比当前线程更早地请求获取锁，因此需要等待前驱线程获取并释放锁后才能继续获取锁
-- 这里看源码hasQueuedPredecessors应该是检查，而非入队

观察公平和非公平锁在获取锁时的区别
参见：FairAndUnfairTest。其中ReentrantLock2，主要公开了getQueuedThreads方法，返回正在等待获取锁的线程列表。
从结果可看出，公平性锁，每次都是从同步队列中的第一个节点获取锁，而非公平性锁出现了一个线程连续获取锁的情况
因为，nonfairTryAcquire中，当一个线程请求锁时，只要获取了同步状态即成功获取锁，在这个前提下，刚释放锁的线程再次获取
同步状态的几率非常大(因为其他线程还得从阻塞变成非阻塞再尝试竞争)，使得其他线程只能在同步队列中等待

非公平性锁可能使线程"饥饿"，但由于切换次数少，产生的开销更小。
公平性锁保证了锁的获取按照FIFO原则，而代价是进行大量的线程切换。非公平性锁虽然可能造成"饥饿"，但极少的线程切换，保证了
其更大的吞吐量

### 读写锁
同一时刻可以允许多个读线程访问，但在写线程访问时，所有的读线程和其他写线程均被阻塞。
维护了一对锁，一个读锁一个写锁，通过分离读锁和写锁，使得并发性比一般的拍他锁有了很大的提升

除了保证写操作对读操作的可见性以及并发型的提升外，读写锁能简化读写交互场景的编程方式。
例如共享的缓存数据结构，大部分时间提供读，而写操作占用时间少，但写操作完成后的更新需要对后续的读服务可见。
jdk5之前，只能用java的等待通知机制，当写操作开始时，所有晚于写操作的读操作均会进入等待状态，只有写操作完成并通知后，所有
等待的读操作才能继续执行(写操作之间依靠synchronized)，这样做的目的是使读操作能读取到正确的数据，不会出现脏读。
改用读写锁，则只需在读操作时获取读锁，写操作时获取写锁即可。

一般，读写锁的性能会比排他锁更好，因为大多数场景读多于写，此时读写锁能提供比排他锁更好的并发性和吞吐量。

ReentrantReadWriteLock特性：
+ 公平性选择。支持公平、非公平
+ 重进入。
支持。读线程获取读锁后，能再次获取读锁。写线程获取写锁后能再次获取写锁，同时也可以获取读锁
+ 锁降级
遵循，获取写锁、获取读锁再释放写锁的次序，写锁能降级成为读锁

1. 读写锁的接口与示例
ReentrantReadWriteLock展示内部工作状态的方法：
+ getReadLockCount  --所有线程获取读锁(包括重入)的次数
返回当前读锁被获取的次数。如，一个线程，连续获取(重进入)了n次读锁，那么占据读锁的线程数是1，而此方法返回n
+ getReadHoldCount
返回当前线程获取读锁的次数。
+ isWriteLocked
判断写锁是否被获取
+ getWriteHoldCount
返回当前写锁被获取的次数

展示读写锁的使用方式
参见：Cache

2. 读写锁的实现分析
1)读写状态的设计
ReentrantLock中同步状态表示锁被一个线程重复获取的次数，
读写锁的自定义同步器需要在同步状态(一个整形变量)上维护多个读线程和一个写线程的状态。

若在一个整形变量上维护多种状态，就一定需要"按位切割使用"这个变量，读写锁将变量切分成了两个部分，高16位表示读，低16位表示写
0000000000000001 0000000000000011
上述表示一个线程已经获取了写锁，且重进入了两次，同时也连续获取了两次读锁。

读写锁通过按位运算，迅速确定读和写各自状态。
设当前同步状态值为S，
+ 写状态等于：S&0x0000FFFF(将高16位全部抹去)。
写状态增加1，等于S+1
+ 读状态等于：S>>16(无符号补0右移16位)。
读状态增加1，等于S+(1<<16)，即S + 0x00010000

根据状态的划分能得出一个推论：S不等于0时，当写状态(S&0x0000FFFF)等于0时，则读状态(S>>16)大于0，即读锁已被获取

2)写锁的获取与释放
写锁是一个支持重入的排他锁。
若当前线程已经获取了写锁，则增加写状态。
若当前线程在获取写锁时，读锁已经被获取(读状态不为0)或该线程不是已经获取写锁的线程，则当前线程进入等待状态。
```java
// reetrantReadWriteLock
protected final boolean tryAcquire(int acquires){
  Thread current = Thread.currentThread();
	int c = getState();
	int w = exclusiveCount(c);
	if(c != 0){  // 状态已是读或写
	  // 存在读锁 或 当前获取线程不是已经获取写锁的线程
	  if(w == 0 || current != getExclusiveOwnerThread()){
		  return false;
		}
		if(w + exclusiveCount(acquires) > MAX_COUNT){
		  throw new Error("Maximum lock count exceeded");
		}
		setState(c + acquires);
		return true;
	}
	if(writerShouldBlock() || !compareAndSetState(c, c+acquires)){
	  return false;
	}
	setExclusiveOwnerThread(current);
	return true;
}
```
若存在读锁，则写锁不能被获取，原因在于：读写锁要确保写锁的操作对读锁可见。
若允许读锁在已被获取的情况下再对写锁的获取，那么正在运行的其他读线程就无法感知到当前写线程的操作。
因此，只有等待其他线程都释放了读锁，写锁才能被当前线程获取，而写锁一旦被获取，则其他读线程的后续访问均被阻塞。

写锁的释放与ReentrantLock的释放类似，每次均减少写状态，当写状态为0时表示写锁已被释放，从而等待的读写线程能继续访问读写锁，
同时本次写线程的修改对后续读写线程可见。

3)读锁的获取与释放
读锁是一个支持重入的共享锁，能被多个线程同时获取，在没有其他写线程访问(或写状态为0)时，读锁总会被成功地获取，而所做的也只是(
线程安全的)增加读状态。

若当前线程在获取读锁时，写锁已被其他线程获取，则进入等待状态。

读状态是所有线程获取读锁次数的总和，而每个线程各自获取读锁的次数只能选择保存在ThreadLocal中，由线程自身维护。
```java
// reetrantReadWriteLock
protected final int tryAcquireShared(int unused){
  for(;;){
	  int c = getState();
		int nextc = c + (1<<16);
		if(nextc < c){
		  throw new Error("Maximum lock count exceeded");
		}
		if(exclusiveCount(c)!=0 && owner != Thread.currentThread()){  // 写锁已被人获取
		  return -1;
		}
		if(compareAndSetState(c, nextc)){  // 成功获取锁
		  return 1;
		}
	}
}
```
若其他线程已获取了写锁，则当前线程获取读锁失败，进入等待状态。
若当前线程获取了写锁或写锁未被锁定，则当前线程(线程安全，依靠CAS保证)增加读状态，成功获取读锁

读锁的每次释放(线程安全，可能有多个读线程同时释放读锁)均减少读状态，减少的值是(1<<16)

4)锁降级
若当前线程拥有写锁，然后将其释放，最后再获取读锁，这种分段完成的过程不能称为锁降级。
锁降级指，把该有的读锁先释放(若之前持有)，把持住(当前拥有的)写锁，再获取读锁，随后释放(先前拥有的)写锁的过程

场景举例：因为数据不常变化，多线程并发进行数据处理，当数据变更后，若当前线程感知到数据变化，则进行数据的准备工作，同时其他
处理线程被阻塞，直到当前线程完成数据的准备工作。
```java
private volatile boolean update = false;
public void processData(){
  readLock.lock();
	if(!update){  // 感知到数据变更
	  // 必须先释放读锁
		readLock.unlock();
		// 锁降级从写锁获取到开始，锁降级-持有写锁
		writeLock.lock();
		try{
		  if(!update){
			  // 准备数的流程(略)...
				update = true;
			}
			readLock.lock();  // 锁降级-获取读锁
		}finally{
		  writeLock.unlock();  // 锁降级-释放写锁
		}
		// 锁降级完成，写锁降级为读锁
	}
	// 使用数据的流程(略)
	try{
	  // 使用数据的操作..
	}finally{
	  readLock.unlock();
	}
}
```
当数据变更后，update变量被设置为false，此时，所有访问processData方法的线程都能感知到，但只有一个线程能获取到写锁，其他线程
会被阻塞在读锁和写锁的lock上。
当前线程获取写锁完成数据准备后，再获取读锁，随后释放写锁，完成锁降级

锁降级中读锁的获取是有必要的，主要是为了保证数据的可见性。
若当前线程不获取读锁而是直接释放写锁，还未执行到`使用数据的流程(略)`，假设此刻另一个线程(T)获取了写锁并修改了数据，
那么当前线程无法感知到线程T的数据更新。  -- 因为当前线程下一步就准备使用数据了，而没有任何的锁保证
若当前线程获取读锁，即遵循锁降级的步骤，则线程T将会被阻塞，直到当前线程使用数据并释放读锁之后，
线程T才能获取写锁进行数据更新。--这就是让当前线程执行完最后的`readLock.unlock()`，T才能进行写

ReentrantReadWriteLock不支持锁升级(把持读锁、获取写锁，最后释放读锁的过程)。目的也是保证数据的可见性，
若读锁已被多个线程获取，其中任意线程成功获取了写锁并更新了数据，则其更新对其他获取到读锁的线程是不可见的

### LockSupport工具
定义了一组公共静态方法，提供了最基本的线程阻塞和唤醒功能。

LockSupport提供的阻塞和唤醒方法：
+ park
阻塞当前线程，若调用unpark或当前线程被中断，则从park返回
+ parkNanos
阻塞当前线程，最长不超过nanos纳秒，返回条件在park基础上增加了超时返回
+ parkUntil
阻塞当前线程，直到deadline时间(从1970年开始到deadline时间的毫秒数)
+ unpark
唤醒处于阻塞状态的线程

jdk6增加park(Object blocker)、parkNanos(Object blocker, long nanos)、parkUntil(Object blocker, long deadline)，
用以替代原有的park方法
参数blocker用来标识当前线程在等待的对象(称阻塞对象)，主要用于问题排查和系统监控
dump日志中多了`parking to wait for <xxx>(a com.xxx)`


### Condition接口
任意一个java对象，都拥有一组监视器方法(定义在Object上)，包括wait、notify、notifyAll，与synchronized配合，实现等待/通知模式

Object的监视器方法与Condition接口的对比
对比项  Object Monitor Methods  Condition
前置条件  获取对象的锁  调用Lock.lock后调用Lock.newCondition获取Condition对象
调用方式  object.wait  condition.await
等待队列个数  一个  多个
当前线程释放锁并进入等待状态  支持  支持
当前线程释放锁并进入等待状态，在等待状态中不响应中断  不支持  支持
当前线程释放锁并进入超时等待状态  支持  支持
当前线程释放锁并进入等待状态到将来的某个时间  不支持  支持
唤醒等待队列中的一个线程  支持  支持
唤醒等待队列中的全部线程  支持  支持

1. Condition接口与示例
Condition是依赖Lock对象的
使用方式参见：ConditionUseCase

Consition的(部分)方法及描述
+ await
当前线程进入等待状态，直到被通知(signal)或中断后，当前线程将进入运行状态且从await返回的情况包括：
  + 其他线程调用该Condition的signal或signalAll，当前线程被选中唤醒，若从await返回，那表明该线程已经获取了Condition对象
	所对应的锁
	+ 其他线程(调用interrupt)中断当前线程
+ awaitUninterruptibly
当前线程进入等待状态直到被通知，对中断不敏感
+ awaitUntil
当前线程进入等待状态，直到被通知、中断或到某个时间。若没到指定时间就被通知，返回true，否则表示到了指定时间，返回false
+ signal
唤醒一个等待在Condition上的线程，该线程从等待方法返回前必须获得与Condition相关的锁
+ signalAll
唤醒所有等待在Condition上的线程，能从等待方法返回的线程必须获得与Condition相关联的锁

通过有界队列示例了解Condition，参见BoundedQueue

2. Condition的实现分析
Condition是接口，ConditionObject是实现。
ConditionObject是AbstractQueuedLongSynchronizer的内部类，因为Condition的操作需要获取相关的锁，所以作为同步器的
内部类也较为合理。
每个Condition对象都包含着一个队列(称为等待队列)，是Condition对象实现等待/通知功能的关键

下面分析Condition的实现，若不加说均指ConditionObject
1)等待队列
是一个FIFO的队列，在队列的每个节点都包含了一个线程引用，就是在Condition对象上等待的线程。
若一个线程调用了Condition.await，那么该线程将会释放锁、构造成节点加入等待队列并进入等待状态。
同步队列和等待队列中的节点类型都是AbstractQueuedSynchronizer.Node

一个Condition包含一个等待队列，Condition拥有首节点(firstWaiter)和尾节点(lastWaiter)。
当前线程调用Condition.await，将会以当前线程构造节点，并将节点从尾部加入等待队列。

Condition拥有首、尾节点的引用，新增节点只需将原有的尾节点nextWaiter指向它，并更新尾节点即可。没有CAS，因为调用await的线程
已经获取了锁。

在Object的监视器模型上，一个对象拥有一个同步队列和等待队列。
Lock(准确地说是同步器)拥有一个同步队列和多个等待队列

Condition的实现时同步器的内部类，因此每个Condition实例都能访问同步器的方法，都拥有所属同步器的引用

2)等待
调用Conditioin的awaitxx方法，会使当前线程进入等待队列并释放锁，同时线程状态变为等待状态。
当从await返回时，当前线程一定获取了Codition相关联的锁

从队列(同步队列和等待队列)的角度看，当调用await时，相当于同步队列的首节点(获取了锁的节点)移动到Condition的等待队列中。
```java
// ConditionObject
public final void await(){
  if(Thread.interrupted()){
	  throw new InterruptedException()
	}
	// 当前线程加入等待队列
	Node node = addConditionWaiter();
	// 释放同步状态，释放锁
	int savedState = fullyRelease(node);
	int interruptMode = 0;
	while(!isOnSyncQueue(node)){  // 不在同步队列，即在等待队列中
	  LockSupport.park(this);  // 休眠
		if((interruptMode = checkInterruptWhileWaiting(node)) != 0){  // 被唤醒时检查是否被interrupt
		  break;
		}
	}
	// 获取同步队列的锁(可能成功或阻塞)，
	if(acquireQueued(node, saveState) && interruptMode != THROW_IE){
	  interruptMode = REINTERRUPT;
	}
	// 连接到后续的未取消的节点
	if(node.nextWaiter != null){
	  unlinkCancelledWaiters();
	}
	// 抛异常或重新设定interrupt
	if(interruptMode != 0){
	  reportInterruptAfterWait(interruptMode);
	}
}
```
调用该方法的线程，是成功获取了锁的，即同步队列的首节点。
该方法会将当前线程构造成节点并加入等待队列中，释放同步状态，唤醒同步队列中的后继节点，然后当前线程进入等待状态。

当等待队列中的节点被唤醒，则唤醒节点的线程开始尝试获取同步状态。
若其他线程不是通过Condition.signal唤醒，而是对等待线程进行中断，则抛出InterruptedException

同步队列的首节点并不会直接加入等待队列，而是通过addConditionWaiter把当前线程构造成一个新的节点加入等待队列中

3)通知
调用Condition.signal方法，将会唤醒在等待队列中等待时间最长的节点(首节点)，在唤醒前，会将节点移到同步队列中。
```java
// ConditionObject
public final void signal(){
  if(!isHeldExclusively()){
	  throw new IllegalmonitorStateException();
	}
	Node first = firstWaiter;
	if(first != null){
	  doSignal(first);
	}
}
```
调用该方法的前置条件是，当前线程获取了锁。
获取等待队列的首节点，将其移动到同步队列并使用LockSupport唤醒节点中的线程

调用同步器的enq方法，等待队列中的头结点线程地移动到同步队列。
移动后，当前线程再用LockSupport唤醒该节点的线程

被唤醒后的线程，将从await中的while循环中退出(isOnSyncQueue返回true，节点已在同步队列中)，进而调用同步器的acquireQueued
加入到获取同步状态的竞争中。

成功获取同步状态(锁)后，被唤醒的线程将从先前调用的await方法返回，此时线程已经成功获取锁

Condition.signalAll相当于对等待队列中的每个节点均进行一次signal，效果是将等待队列中所有节点全部移动到同步队列中，并唤醒
每个节点的线程

小结：
只有理解这些API和组件的实现细节才能更加准确地运用它们


## 第6章，java并发容器和框架
Doug Lea为java开发者提供并发容器和框架

### ConcurrentHashMap的实现原理与使用
是线程安全且高效的HashMap。

1. 为什么要用ConcurrentHashMap
+ 线程不安全的HashMap
多线程下，用HashMap.put会引起死循环，导致CPU利用率100%。
因为多线程会导致HashMap的Entry链表形成环形结构，一旦形成，Entry的next节点永远不为空，产生死循环获取Entry
+ 效率低下的HashTable
用synchronized，但在线程竞争激烈的情况下，效率非常低。因为一个线程访问时，其他线程会进入阻塞。
+ ConcurrentHashMap的锁分段技术可有效提升并发访问率
容器中有多把锁，每一个用于锁定容器的一部分数据，从而有效提升并发访问效率。
将数据分成一段段地存储，而后给每一段数据分配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被
其他线程访问

2. ConcurrentHashMap的结构
由Segment数组结构和HashEntry数组结构组成。
Segment是一种可重入锁(ReentrantLock)，在ConcurrentHashMap中扮演锁的角色
HashEntry用于存储kv对数据。
一个ConcurrentHashMap包含一个Segment数组。
Segment结构和HashMap类似，是一种数组和链表结构，包含一个HashEntry数组，每个HashEntry是一个链表结构的元素，
每个Segment守护者一个HashEntry数组里的元素。

3. ConcurrentHashMap的初始化
1)初始化segments数组
```java
if(concurrencyLevel > MAX_SEGMENTS){
  concurrencyLevel = MAX_SEGMENTS;
	int sshift = 0;
	int ssize = 1;
	while(ssize < concurrencyLevel){  // 以2的幂次方式进行扩容，直到大于concurrencyLevel
	  ++shift;  // 从1开始
		ssize <<= 1;  // 向左移动1位，*2，
	}
	segementShift = 32 - sshift;
	segmentMask = ssize - 1;  // 掩码，-1后除了第一位0，其他都是1
	this.segments = Segment.newArray(ssize);
}
```
segments数组的长度ssize是通过concurrencyLevel计算得出。
为了能通过按位与的散列算法定位segments数组的索引，必须保证segments数组的长度是2的N次方(power-of-two size)，
计算出一个大于或等于concurrencyLevel的最小的2的N次方值。

2)初始化segmentShift和segmentMask
这俩变量需要在定位segment时的散列算法中使用，sshift等于ssize从1向左移动位的次数，默认concurrencyLevel=16，
1需要向左移动4次，所以sshift=4
segmentShift用于定位参与散列运算的位数，segmentShift等于32-sshift，这里等于28。用32是因为ConcurrencyHashMap
中的hash方法输出的最大数是32位的。
segmentMask是散列运算的掩码，等于ssize-1，即15，掩码的二进制各个位的值都是1.
因为ssize最大长度是65536，所以segmentShift最大值是16，segmentMask最大值是65535，对应的二进制数是16位，每个位1

3)初始化每个segment
参数initialCapacity是ConcurrentHashMap的初始化容量，loadfactor是每个segment的负载因子
通过这俩参数初始化数组中的每个segment
```java
if(initialCapacity > MAXIMUM_CAPACITY){
  initialCapacity = MAXIMUM_CAPACITY;
	int c = initialCapacity / ssize;
	if(c * ssize < initialCapacity){
	  ++c;
	}
	int cap = 1;  // 是segment里的HashEntry数组的长度
	while(cap < c){
	  cap <<= 1;
	}
	for(int i = 0; i < this.segments.length; ++i){
	  this.segments[i] = new Segment<K,V>(cap, locaFactor);
	}
}
```
cap等于initialCapacity除以ssize的倍数c，若c大于1，就会取大于等于c的2的N次方值，所以cap不是1，就是2的N次方。
segment的容量threhold=(int)cap*loadFactor，默认下initialCapacity=16，loadfactor=0.75，cap=1，threshold=0

4. 定位Segment
先用Wang/Jenkins hash的变种算法对元素的hashCode进行一次再散列
```java
private static int hash(int h){
  h += (h<<15)^Oxffffcd7d;
	h ^= (h>>>10);
	h += (h<<3);
	h ^= (h>>>6);
	h += (h<<2) + (h<<14);
	return h^(h>>>16);
}
```
目的是减少散列冲突，使元素能均匀地分布在不同的Segment上，从而提高容器的存取效率。

若不通过再散列而直接执行散列计算
syso(Integer.parseInt("0001111", 2) & 15);
syso(Integer.parseInt("0011111", 2) & 15);
syso(Integer.parseInt("0111111", 2) & 15);
syso(Integer.parseInt("1111111", 2) & 15);
得到结果都是15，若不进行再散列，散列冲突会非常严重。只要低位一样，无论高位是什么，其散列值总是一样。

再把上面的二进制数进行再散列
0100|0111|0110|0111|1101|1010|0100|1100
1111|0111|0100|0011|0000|0001|1011|1000
0111|0111|0110|1001|0100|0110|0011|1110
1000|0011|0000|0000|1100|1000|0001|1010
可发现，每一位的数据都散列开了，通过这种再散列能让数字的每一位都参加到散列运算中，减少散列冲突。

定位segment
```java
final Segment<K,V> segmentFor(int hash){  // 再hash过的hashCode
  return segemnts[(hash>>segmentShift) & segmentMask];
}
```
默认segmentShift=28，segmentMask=15，再散列后的数最大是32位的二进制数据，向右无符号移动28位，意思是让高4位
参与到散列运算中，(hash>>segmentShift)&segmentMask的结果分别是4、15、7和8(4个数的前4位)，可看到散列值没有发生冲突

5. ConcurrentHashMap的操作
1)get
先经过一次再散列，然后用这个值通过散列运算定位到Segment，在通过散列算法定位到元素
```java
public V get(Object key){
  int hash = hash(key.hashCode());
	return segmentFor(hash).get(key, hash);
}
```
get操作的高效之处在于整个过程不用加锁，除非读到的值是空才会加锁重读。原因是他的get方法里将要使用的共享变量都定义
成volatile类型，如，用于统计当前Segment大小的count和用于存储值的HashEntry的value。

定义成volatile的变量，能在线程间保持可见性，能被多线程同时读，保证不会读到过期的值，但只能被单线程写(有一种情况
可悲多线程写，即写入的值不依赖于原值)，在get里只需要读不需要写共享变量count和value，所以不用加锁。
不会读到过期值，是因为根据jmm的happen before原则，对volatile字段的写入操作先于读操作，即使两个线程同时修改和
获取volatile变量，get也能拿到最新的值，这是用volatile替换锁的经典应用场景
```
transient volatile int count;
volatile V value;
```

定位元素时，定位HashEntry和定位Segment的散列算法虽然一样，都与素组的长度-1再相"与"，但相"与"的值不一样，定位
Segment用的是元素的hashCode通过在散列后得到的值的高位，而定位HashEntry直接用的是再散列后的值。
目的是避免两次散列后的值一样，虽然元素再Segment里散列开了，但却没有在HashEntry里散列开
```
(hash >>> segmentShift) & segmentMask  // 定位Segment
int index = hash & (tab.length - 1);  // 定位HashEntry
```

2)put操作
由于put里要对共享变量进行写入操作，为线程安全，在操作共享变量时必须加锁。
先定位到Segment，然后在Segment里进行插入。
插入先判断是否需要对Segment里的HashEntry数组进行扩容，然后定位添加元素的位置，将其放入HashEntry数组里
a. 是否需要扩容
插入前，先判断Segment里的HashEntry数组是否超过容量(threshold)，若超过，则对数组进行扩容。
比HashMap更恰当，因为HashMap是在插入元素后判断元素是否已经达到容量，若达到了就进行扩容，但很有可能扩容后没有
新元素插入，这时就进行了一次无效的扩容

b. 如何扩容
先创建一个容量是原来容量两倍的数组，然后将元素进行在散列后插入新数组中。只对某个segment进行扩容

3)size操作
要统计整个map的大小，需要对所有Segment里元素大小求和。Segment的全局变量count是一个volatile变量，在多线程
场景下，不能直接将所有count相加，因为可能累加前使用的count发生变化，结果不准了。最安全的做法是把所有Segment的
put、remove和clean全锁住，不过效率低。
因为在累加count操作过程中，之前累加过的count发生变化的几率非常小，所以ConcurrentHashMap的做法是先尝试2次通过
不锁住Segment的方式来统计各个Segment大小，若过程中，count发生变化，则再采用加锁的方式统计。
用modCount变量，在put、remove和clean里操作元素前都将变量modeCount加1，那么在统计size前后比较modCount是否
发生变化，从而得知Segment大小是否发生变化

### ConcurrentLinkedQueue
非阻塞的方式实现线程安全队列
是一个机遇链接节点的无界线程安全队列，采用先进先出的规则对节点进行排序，尾部添加，从头部获取元素。
采用wait-free算法(即CAS)实现，在Michael & Scott算法上进行了一些修改

1. ConcurrentLinkedQueue的结构
由head和tail节点组成，每个节点(Node)由节点元素(item)和指向下一个节点(next)的引用组成，节点间通过next关联，
组成一张链表结构的队列。
默认head存储的元素为空，tail等于head
private transient volatile Node<E> tail = head;

2. 入队列
1)入队列的过程
添加到尾部。
通过调试入队过程观察head和tail变化，发现入队主要做两件事：
+ 将入队节点设置成当前队列尾节点的下一个节点
+ 更新tail节点，
  + 若tail节点的next节点不为空，则将入队节点设置成tail节点
	+ 若tail节点的next节点为空(即已经是尾部)，则将入队节点设置成tail的next节点，所以tail并不总是尾节点

多线程同时入队就可能出现其他线程插队情况。
若一个线程正在入队，他必须先获取尾节点，然后设置尾节点的下一个节点为入队节点，但这时可能有另一个线程插队，那么队列
的尾节点发生变化，这时当前线程要暂停入队，然后重新获取尾节点。
```java
public bool offer(E e){
  if(e == null) throw new NullPointerException();
	Node<E> n = new Node<E>(e);  // 入队前，创建一个入队节点
	retry:
	for(;;){  // 死循环，入队不成功则反复入队
		Node<E> t = tail;  // 创建一个指向tail节点的引用，用来判断此期间tail是否变更
		Node<E> p = t;  // p用来表示队列的尾节点，默认下等于tail，用来向next移动
		for(int hops = 0; ; hops++){
		  // 获取p的下一个节点
			Node<E> next = succ(p);
			// next节点不为空，说明p不是尾节点，需要更新p后再将他指向next节点
			if(next != null){
			  // 循环了两次及以上，并且当前节点还是不等于尾节点
				if(hops > HOPS && t != tail){
				  continue retry;
				}
				p = next;  // 设定为next继续下次for
			}
			// 若p是尾节点，则设置p节点的next为入队节点
			else if(p.casNext(null, n)){
			  // 若tail节点有大于等于1个next节点，则将入队节点设置成tail节点，
				// 更新失败也没关系，因为失败了表示有其他线程成功更新了tail节点
				if(hops >= HOPS){
				  casTail(t, n);  // 更新tail节点，允许失败
				}
				return true;  --这里应该是唯一的出口，只有将此节点设定到tail.next上才可以
			}
			// p有next节点，表示p的next节点是尾节点，则重新设置p节点
			-- 可能是casNext失败了，因为其他线程给next设定了值而本线程cas失败，
			else{
			  p = succ(p);
			}
		}
	}
}
```
整个入队过程主要两件事：定位出尾节点，使用CAS算法将入队节点设置成尾节点的next节点，若不成功则重试。

2)定位尾节点(succ)
tail并不总是尾节点，所以每次入队都必须先通过tail节点来找到尾节点。
尾节点可能是tail也可能是tail的next节点。
代码中循环体中的第一个if就是判断tail是否有next，有则表示next可能是尾节点。
获取tail节点的next节点需要注意的是，p节点等于p的next情况，只有一种可能就是p和p的next都等于空，表示这个队列刚
初始化，正准备添加节点，所以需要返回head节点。

获取p的next如下
```
final Node<E> succ(Node<E> p ){
  Node<E> next = p.getNext();
	return (p == next) ? head: next;
}
```

3)设置入队节点为尾节点
p.casNext(null, n)用于将入队节点设置为当前队列尾节点的next，
+ 若p是null，表示p是当前队列的尾节点， --cas成功
+ 若不为null，表示其他线程更新了尾节点，需要重新获取当前队列的尾节点  --cas失败

4)HOPS的设计意图
对于先进先出的队列，入队要做的事情是将入队节点设置成尾节点，doug lea逻辑有些复杂，

假如用以下方式是否可行?
```java
public boolean offer(E e){
  if(e == null){
	  throw new NullPointerException();
	}
	Node<E> n = new Node<E>(e);
	for(;;){
	  Node<E> t = tail;
		if(t.casNext(null, n) && casTail(t, n)){  // 设定尾部节点.next 和 设定tail
		  return true;
		}
	}
}
```
让tail永远作为队列的尾节点，代码少，逻辑清晰。但有个缺点，每次需要用循环CAS更新tail节点。
若能减少CAS更新tail的次数，就能提高入队的效率，所以doug lea用hops变量来控制并减少tail节点的更新频率，
并不是每次节点入队后都将tail更新成尾节点，而是当tail节点和尾节点的距离大于等于常量HOPS的值(默认1)时才更新
tail节点，tail和尾节点的距离越长。
使用CAS更新tail的次数越少，但距离越长带来的负面效果是每次入队时定位尾节点的时间越长，因为循环体需要多循环一次
来定位出尾节点，但这样仍能提高入队的效率，因为从本质上它通过增加对volatile变量的读操作来减少对volatile变量的
写操作，而对volatile变量的写操作开销要远远大于读操作，所以入队效率有所提升
`private static final int HOPS = 1;`

注意：入队方法永远返回true，不要通过返回值判断入队是否成功

3. 出队列
从队列返回一个节点元素，并清空该节点对元素的引用
并不是每次出队时都更新head节点，当head节点里有元素时，直接弹出head节点里的元素，而不会更新head节点。
只有当head节点里没有元素时，出队操作才会更新head节点。
做法也是通过hops变量来减少使用CAS更新head节点的消耗，从而提高出队效率。
```java
public E poll(){
  Node<E> h = head;  // h用来判定是否变动
	// p表示头节点，需要出队的节点，用来移动
	Node<E> p = h;
	for(int hops = 0; ; hops++){
	  // 获取p节点的元素
		E item = p.getItem();
		// 若p的元素不为空，使用CAS设置p节点引用的元素为null，若成功则返回p节点的元素，本线程poll成功
		if(item != null && p.casItem(item, null)){
		  if(hops>=HOPS){
			  // 将p节点下一个节点设置成head节点
				Node<E> q = p.getNext();
				updateHead(h, (q != null) ? q : p);
			}
			return item;
		}
		// 若头节点的元素为空 或 头结点发生变化，说明头节点已经被另外一个线程修改了，那么获取p节点的下一个节点
		Node<E> next = succ(p);
		// 若p的下一个节点也为空，说明这个队列已经空了
		if(next == null){
		  // 更新头结点
			updateHead(h, p);
			break;
		}
		// 若下一个元素不为空，则将头节点的下一个节点设置成头节点
		p = next;
	}
	return null;
}
```
首先获取头结点的元素，判断是否其元素是否为空，若为空，表示另外一个线程已经进行了一次出队操作将该节点的元素取走，
若不为空，则用cas将头结点的引用设置成null，若cas成功，则直接返回头节点的元素，若不成功，表示另一个线程已进行了
一次出队操作更新了head节点，导致元素发生变更，需要重新获取头节点

### java中的阻塞队列
1. 什么是阻塞队列
Blocking Queue是一个支持两个附加操作的队列。支持阻塞的插入和移除
+ 支持阻塞的插入方法：当队列满时，队列会阻塞插入元素的线程，直到队列不满
+ 支持阻塞的移除方法：当队列为空时，获取元素的线程会等待队列变为非空

阻塞队列常用于生产者和消费者的场景。

插入和移除操作的4种处理方式：
方法/处理方式  抛出异常  返回特殊值  一直阻塞  超时退出
插入方法  add  offer  put  offer(e, time, unit)
移除方法  remove  poll  take  poll
检查方法  element  peek  不可用  不可用

在阻塞队列不可用时，这两个附加操作提供4中处理方式：
+ 抛出异常：当队列满时，若再插入，会抛出IllegalStateException("Queue full")异常。当队列空时，从队列获取元素
抛出NoSuchElementException异常
+ 返回特殊值。
当插入元素时，返回是否插入成功，成功返回true。若是移除方法，则从队列取一个元素，若没有则返回null
+ 一直阻塞。
当队列满时，若put元素，则队列会一直阻塞，直到队列可用或响应中断退出。当队列空时，若消费take则会阻塞住线程，直到
队列不为空
+ 超时退出
当阻塞队列满时，若插入元素，队列会阻塞线程一段时间，若超过了指定时间，生产者会退出

2. java中的阻塞队列
ArrayBlockingQueue：数组结构，有界
LinkedBlockingQueue：链表结构，有界
PriorityBlockingQueue：支持优先级，无界
DelayQueue：优先级队列实现，无界
SynchronousQueue：不存出元素
LinkedTransferQueue：链表结构，无界
linkedBlockingDeque：链表结构，双向

1)ArrayBlockingQueue
用数组实现，有界阻塞队列。按照FIFO原则对元素排序
访问者的公平性是使用可重入锁(ReentrantLock)实现的

2)LinkedBlockingQueue
用链表实现的有界阻塞队列。默认和最大长度为Integer.MAX_VALUE。按照先进先出的原则对元素进行排序

3)PriorityBlockingQueue
支持优先级的无界阻塞队列。
默认元素采用自然顺序升序排列。也可指定Comparator。不能保证同优先级元素的顺序

4)DelayQueue
一个支持延时获取元素的无界阻塞队列。用PriorityQueue实现。元素需要实现Delayed接口，在创建元素时可以指定多久才能
从队列中获取当前元素。只有延迟期满才能从队列中提取元素

应用场景：
+ 缓存系统的设计。
用DelayQueue保存缓存元素的有效期，用一个线程循环查询DelayQueue，一旦能从DelayQueue中获取元素时，表示缓存有效期到了
+ 定时任务调度。
用DelayQueue保存当天将会执行的任务和执行时间，一旦从DelayQueue中获取到任务就开始执行

(1)如何实现Delayed接口
参考ScheduledThreadPoolExecutor里ScheduledFutureTask类的实现。
a.创建对象时，初始化节本数据。用time记录当前对象延迟到什么时候可以用，用sequenceNumber来标识元素在队列中的先后顺序
```
private static final AtomicLong sequencer = new AtomicLong(0);
SchduledFutureTask(Runnable r, V result, long ns, long period){
  super(r, result);
	this.time = ns;
	this.period = period;
	this.sequenceNumber = sequencer.getAndIncrement();
}
```
b.实现getDelay方法，返回当前元素还需延时多长时间，单位ns。
```
public long getDelay(TimeUnit unit){
  return unit.convert(time - now(), TimeUnit.NANOSECONDS);
}
```
c.实现compareTo指定元素的顺序。例如让延时时间最长的放在队列的末尾
```java
public int compareTo(Delayed other){
  if(other == this){  // compare zero ONLY if same object
	  return 0;
	}
	if(other instanceof ScheduledFutureTask){
	  ScheduledFutureTask<?> x = (ScheduledFutureTask<?>)other;
		long diff = time - x.time;
		if(diff<0){
		  return -1;
		}else if(diff > 0){
		  return 1;
		}else if(sequenceNumber < x.sequenceNumber){
		  return -1;
		}else{
		  return 1;
		}
	}
	long d = (getDelay(TimeUnit.NANOSECONDS) - other.getDelay(TimeUnit.NANOSECONDS));
	return (d == 0) ? 0 : ((d<0)? -1:1);
}
```

(2)如何实现延时阻塞队列
当消费者从队列里获取元素时，若元素没有达到延时时间，就阻塞当前线程
```java
// take
long delay = first.getDelay(TimeUnit.NANOSECONDS);
if(delay <= 0){
  return q.poll();
}else if(leader != null){  // 已经有前人等待了
  available.await();  // 字节也等待
}else{
  Thread thisThread = Thread.currentThread();
	leader = thisThread;  // 设定自己为头
	try{
	  available.awaitNanos(delay);
	}finally{
	  if(leader == thisThread){
		  leader = null;
		}
	}
}
```
leader是一个等待获取队列头部元素的线程。若leader不等于空，表示已经有线程在等待获取队列的头元素。所以，用await让
当前线程等待信号。若leader等于空，则把当前线程设置成leader，用awaitNanos让当前线程等待接收信号或等待delay时间

5)SynchronousQueue
是一个不存储元素的阻塞队列。每个put必须等待一个take操作，否则不能继续添加元素
默认线程采用非公平性策略访问队列。
负责把生产者线程处理的数据直接传递给消费者线程。队列本身不存储任何元素，适合传递性场景。
吞吐量高于LinkedBlockingQueue和ArrayBlockingQueue

6)LinkedTransferQueue
一个由链表结构组成的无界阻塞TransferQueue队列。多了tryTransfer和transfer方法
a. transfer方法
若当前有消费者正在等待接收元素(take或带有时间限制的pool)，transfer可以把生产者传入的元素立刻transfer给消费者。
若没有消费者正在等待接收元素，transfer将元素村放在队列的tail节点，并等到该元素被消费者消费才返回。
```
Node pred = tryAppend(s, haveData);  // 试图把存放当前元素的s节点作为tail节点
return awaitMatch(s, pred, e, (how == TIMED), nanos);  // 让CPU自旋等待消费者消费元素
```
因为自旋会消耗CPU，所以自旋一定的次数后用Thread.yield暂停当前正在执行的线程，并执行其他线程

b.tryTransfer方法
用来试探生产者传入的元素是否能直接传给消费者。若没有消费者等待接收元素，返回false。无论消费者是否接收，方法
立即返回，而transfer必须等到消费者消费了才返回

7)LinkedBlockingQueue
一个由链表结构组成的双向阻塞队列。可以从队列的两端插入和移除元素。
用带有First和Last后缀的方法更清楚
初始化时可设置容量，防止其过度膨胀。双向阻塞队列可以运用在"工作窃取"模式中

3. 阻塞队列的实现原理
若队列空，消费者会一直等待，当生产者添加元素时，消费者如何知道当前队列有元素呢？

使用通知模式实现。
当生产者往满的队列里添加元素时会阻塞生产者，当消费者消费了一个队列中的元素后，会通知生产者当前队列可用。
ArrayBlockingQueue用了Condition实现
```java
private final Condition notFull;
private final Condition notEmpty;

public ArrayBlockingQueue(int capacity, boolean fair){
  notEmpty = lock.newCondition();
  notFull = lock.newCondition();
}

public void put(E e ){
  checkNotNull(e);
	final ReentrantLock lock = this.lock;
	lock.lockInterruptibly();
	try{
	  while(count == items.length){
		  notFull.await();
		}
		insert(e);
	}finally{
	  lock.unlock();
	}
}


public E take(){
  final ReentrantLock lock = this.lock;
	lock.lockInterruptibly();
	try{
	  while(count == 0){
		  notEmpty.await();
		}
		return extract();
	}finally{
	  lock.unlock();
	}
}

private void insert(E x){
  items[putIndex] = x;
	putIndex = inc(putIndex);
	++count;
	notEmpty.signal();
}

// 用LockSupport.part实现阻塞
public final void await(){
  if(Thread.interrupted()){
	  throw new InterruptedException();
	}
	Node node = addConditionWaiter();
	int savedState = fulllyRelease(node);
	int interruptMode = 0;
	while(!isOnSyncQueue(node)){
	  LockSupport.park(this);
		if((interruptMode = checkInterruptWithWaiting(node)) != 0){
		  break;
		}
	}
	if(acquireQueued(node, savedState) && interruptmode != THROW_I){
	  interruptMode = REINTERRUPT;
	}
	if(node.nextWaiter != null){  // clean up if cancelled
	  unlinkCancelledWaiters();
	}
	if(interruptMode != 0){
	  reportInterruptAfterWait(interruptMode);
	}
}

// 调用setBlocker先保存一下将要阻塞的线程，然后用unsafe.park阻塞当前线程
public static void park(Object blocker){
  Thread t = Thread.currentThread();
	setBlocker(t, blocker);
	unsafe.park(false, 0L);
	setBlocker(t, null);
}
```
park方法会阻塞当前线程，只有4种情况中一种发生时，该方法才返回：
+ 与park对应的unpark执行或已经执行时，"已经执行"指unpark先执行，然后再执行park的情况
+ 线程被中断时
+ 等待完time参数指定的毫秒数时
+ 异常现象发生时，这个异常现象没有任何原因

park在不同操作系统用不同的方式实现，在Linux下用的是系统方法pthread_cond_wait实现。代码在JVM源码src/os/linux/
vm/os_linux.cpp里的os::PlatformEvent::park
```c++
void os::PlatformEvent::park(){
  int v;
	for(;;){
	  v = _Event;
		if(Atomic::cmpxchg(v-1, &_Event, v) == v) break;
	}
	guarantee(v>=0, "invariant");
	if(v==0){
	  // Do this the hard way by blocking...
		int status = pthread_mutex_lock(_mutex);
		assert_status(status==0, status, "mutex_lock");
		guarantee(_nParked == 0, "invariant");
		++ _nParked;
		while(_Event < 0){
		  status = pthread_cond_wait(_cond, _mutex);
			// for some reason, under 2.7 lwp_cond_wait() may return ETIME ...
			// Treat this the same as if the wait was interrupted
			if(status == ETIME) {status = EINTR;}
			assert_status(status==0|| status == EINTR, status, "cond_wait");
		}
		-- _nParked;

		// In theory we could move the ST of 0 into _Event past the unlock(),
		// but then we'd need a MEMBAR after the ST.
		_Event = 0;
		status = pthread_mutex_unlock(_mutex);
		assert_status(status==0, status, "mutex_unlock");
	}
	guarantee(_Event >= 0, "invariant");
}
```
pthread_cond_wait是一个多线程的条件变量函数，cond可理解为线程在等待一个条件发生，这个条件是一个全局变量。
此方法接收两个参数：一个共享变量_cond，一个互斥量_mutex。
unpark在linux下用pthread_cond_signal实现。

当线程被阻塞队列阻塞时，线程进入WAITING(parking)状态

### Fork/Join框架
1. 什么是Fork/Join框架
一个并行执行任务的框架，是一个把大任务分割成若干个小人物，最终汇总每个小任务结果后得到大任务结果的框架

2. 工作窃取算法
work-stealing指某个线程从其他队列里窃取任务来执行。
干完活的线程去帮助其他线程干活，会访问同一个队列，为了减少窃取任务线程和被窃取任务线程之间的竞争，通常会用
双端队列，被窃取任务线程从头，而窃取任务从尾拿任务

工作窃取算法
+ 优点：充分利用线程进行并行计算，减少了线程间的竞争
+ 缺点：在某些情况下还是存在竞争，比如双端队列中只有一个任务时，并且该算法会消耗更多的系统资源，如创建多个线程和
多个双端队列

3. Fork/Join框架的设计
如何设计一个Fork/Join框架？
+ 分割任务。
fork类把大任务分割成子任务，有可能还需要继续分，直到子任务足够下
+ 执行任务并合并结果。
分割的子任务分别放在双端队列里，然后几个启动线程分别从双端队列里获取任务执行。子任务执行完的结果都统一放在一个
队列里，启动一个线程从队列里拿数据，然后合并这些数据

Fork/Join使用两个类完成以上两件事情
1)ForkJoinTask：要使用ForkJoin框架，必须先创建一个ForkJoin任务。它提供在任务中执行fork和join操作的机制。
框架提供了两个子类
+ RecursiveAction：用于没有返回结果的任务
+ RecursiveTask：用于有返回结果的任务
2)ForkJoinPool：ForkJoinTask需要通过ForkJoinPool来执行
任务分割出的子任务会添加到当前工作线程所维护的双端队列中，进入队列的头部。
当一个工作线程的队列里暂时没有任务时，它会随机从其他工作线程的队列的尾部获取一个任务

4. 使用Fork/Join框架
需求：计算1+2+3+4
先考虑是如何分割任务，若希望每个子任务最多执行两个数的相加，那设置分割的阈值为2，由于是4个数相加，所以Fork/Join框架
会把这个任务fork成两个子任务，子任务一计算1+2，子任务二计算3+4，然后再join两个子任务的结果。
参见：CountTask
每个子任务内在调用fork时，又会进入compute方法，看看当前子任务是否需要继续分割子任务，若不用则执行当前任务并返回结果
用join会等待子任务执行完并得到其结果

5. Fork/Join框架的异常处理
ForkJoinTask在执行时可能抛出异常，而在主线程里没办法直接捕获，ForkJoinTask提供了isCompletedAbnormally检查任务
是否已经抛出异常或已经被取消了，可通过ForkJoinTask的getException获取异常

6. Fork/Join框架的实现原理
ForkJoinPool由ForkJoinTask数组和ForkJoinWorkerThread数组组成
ForkJoinTask数组负责将存放程序提交给ForkJoinPool的任务，ForkJoinWorkerThread数组负责执行这些任务

1)ForkJoinTask的fork方法实现原理
调用fork时，程序会调用ForkJoinWorkerThread的pushTask方法异步地执行这个任务，然后立即返回结果
```
public final ForkJoinTask<V> fork(){
  ((ForkJoinWorkerThread)Thread.currenThread()).pushTask(this);
	return this;
}
```
pushTask把当前任务存放在ForkJoinTask数组队列里。在调用ForkJoinPool的signalWork唤醒或创建一个工作线程来
执行任务
```java
final void pushTask(ForkJoinTask<?> t){
  ForkJoinTask<?>[] q;
	int s, m;
	if((q = queue) != null){  // ignore if queue removed
	  long u = (((s = queueTop) & (m = q.length - 1)) << ASHIFT) + ABASE;
		UNSAFE.putOrderdObject(q, u, t);
		queueTop = s + 1;  // or use putOrderedInt
		if((s -= queueBaes) <= 2){
		  pool.signalWork();
		}else if(s == m){
		  growQueue();
		}
	}
}
```

2)ForkJoinTask的join方法实现原理
主要作用是阻塞当前线程并等待获取结果。
```java
public final V join(){
  if(doJoin() != NORMAL){
	  return reportResult();
	}else{
	  return getRawResult();
	}
}
private V reportResult(){
  int s;
	Throwable ex;
	if((s = status) == CANCELLED){
	  throw new CancellationException();
	}
	if(s == EXCEPTIONAL && (ex = getThrowableException()) != null){
	  UNSAFE.throwException(ex);
	}
	return getRawResult();
}
```
当前任务状态有4种：已完成(NORMAL)、被取消(CANCELLED)、信号(SIGNAL)、出现异常(EXCEPTIONAL)

通过doJoin得到当前任务的状态，判断返回什么结果：
+ 若任务状态是已完成，直接返回任务结果
+ 若任务状态是被取消，直接抛出CancellationException
+ 若任务状态是抛出异常，则直接抛出对应的异常

```
private int doJoin(){
  Thread t;
	ForkJoinWorkerThread w;
	int s;
	boolean completed;
	if((t = Thread.currentThread()) instanceof ForkJoinWorkerThread){
	  if((s = status) < 0){  // 任务执行完成
		  return s;
		}
		if((w = (ForkJoinWorkerThread)t).unpushTask(this)){  // 取出任务并执行
		  try{
			  completed = exec();
			}catch(Throwable rex){
			  return setExceptionalCompletion(rex);  // 记录异常，并设状态为EXCEPTIONAL
			}
			if(completed){
			  return setCompletion(NORMAL);  // 顺利执行完成
			}
		}
		return w.joinTask(this);
	}else{
	  return externalAwaitDone();
	}
}
```

## 第7章，java中的13个原子操作类
当程序更新一个变量时，若多线程同时更新这个变量，可能值会有期望外的效果。
atomic包的原子操作类提供了一种用法简单、性能高效、线程安全地更新一个变量的方式

### 原子更新基本类型类
方法：
+ addAndGet。
原子方式将输入的值与实例中的值相加，返回结果
+ compareAndSet
若输入的值等于预期值，以原子方式将新值设定
+ getAndIncrement
已原子方式将当前值+1，返回自增前的值
+ lazySet
最终设置成newValue。
可能导致其他线程在之后的一小段时间内还可以读到旧的值。
+ getAndSet
以原子方式设置为newValue，返回旧值

示例代码：AtomicIntegerTest

```
public final int getAndIncrement(){
  for(;;){
	  int current = get();  // 取得值
		int next = current + 1;  // +1
		if(compareAndSet(current, next)){  // cas进行原子更新
		  return current;
		}
	}
}

public final boolean compareAndSet(int expect, int update){
  return unsafe.compareAndSwapInt(this, valueOffset, expect, update);
}
```

Atomic包里的类基本都用Unsafe实现
```
public final native boolean compareAndSwapObject(Object o, long offset, Object expected, Object x);
public final native boolean compareAndSwapInt(Object o, long offset, int expected, int x);
public final native boolean compareAndSwapLong(Object o, long offset, long expected, long x);
```
看AtomicBoolean，将Boolean转换成整形，在用compareAndSwapInt
原子更新char、float和double都是类似

### 原子更新数组
常用方法：
+ addAndGet(int i, int delta)
原子方式将输入值域数组中索引i的元素相加
+ compareAndSet(int i, int expect, int update)
若当前值等于预期值，以原子方式将数组位置i的元素设置成update值

用例：AtomicIntegerArrayTest

### 原子更新引用类型
若要原子更新多个变量，需要用原子更新引用类型提供的类
AtomicReference：原子更新引用类型
AtomicReferenceFieldUpdater：原子更新引用类型里的字段
AtomicMarkableReference：原子更新带有标记位的引用类型
可原子更新一个布尔类型的标记位和引用类型

用例：AtomicReferenceTest

### 原子更新字段类
原子更新某个类里的某个字段
AtomicIntegerFieldUpdater/AtomicLongFieldUpdater
AtomicStampedReference：原子更新带有版本号的引用类型。
将整数值与引用关联起来，可用于原子的更新数据和数据的版本号，可解决cas进行原子更新时可能出现的ABA问题

要原子更新字段：
1)使用静态方法newUpdater创建一个更新器，设置想要更新的类和属性
2)更新类的字段(属性)必须用public volatile修饰符

参见：AtomicIntegerFieldUpdaterTest

## 第8章，java中的并发工具类

### 等待多线程完成的CountDownLatch
允许一个或多个线程等待其他线程能完成操作

主线程等待所有线程完成sheet的解析操作，用join
参见：JoinCountDownLatchTest
join用于让当前执行线程等待join线程执行结束。原理是不停检查join线程是否存活，若join线程存活则让当前线程永远等待。
wait(0)表示永远等待
```
while(isAlive()){
  wait(0);
}
```
直到join线程中止后，线程的this.notifyAll会被调用，是在jvm里调用。

jdk5提供CountDownLatch也可以实现join功能
参见：CountDownLatchTest

构造函数，想等待N个点完成，就传入N
调用countDown时，N减1，await方法会阻塞当前线程，直到N==0。

一个线程调用countDown方法happen-before，另一个线程调用await方法

### 同步屏障CyclicBarrier
他要做的是，让一组线程达到一个屏障(同步点)时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程
才会继续执行

1. CyclicBarrier简介
默认构造CyclicBarrier(int parties)，参数表示屏障拦截的线程数量，每个线程调用await告诉CyclicBarrier我已到达
了屏障，然后当前线程被阻塞。
参见：CyclicBarrierTest

所有线程到达屏障时，优先执行barrierAction
参见：CyclicBarrierTest2

2. CyclicBarrier应用场景
用于多线程计算数据，最后合并结果的场景
参见：BankWaterService

3. CyclicBarrier和CountDownLatch的区别
CountDownLatch的计数器只能用一次，而CyclicBarrier的计数器可以用reset重置。
CyclicBarrier可用于处理更复杂的业务场景，例如，若计算发生错误，可以重置计数器，并让线程重新执行一次
还有其他方法，参见：CyclicBarrierTest3

### 控制并发线程数的Semaphore(信号量)
用来控制同时访问特定资源的线程数量，通过协调各个线程，保证合理的使用公共资源

1. 应用场景
可用于流量控制，特别是公用资源有限的场景，如数据库连接
参见：SemaphoreTest

其他方法：
+ availablePermits：返回此信号量中当前可用的许可证数
+ gerQueueLength：返回正在等待获取许可证的线程数
+ hasQueuedThreads：是否有线程正在等待获取许可证
+ reducePermits：减少reduction个许可证
+ getQueuedThreads：返回所有等待获取许可证的线程集合

### 线程间交换数据的Exchanger(交换者)
一个用于线程间协作的工具类。用于线程间的数据交换。
提供一个同步点，这个点上，两个线程可以交换彼此的数据。
两个线程通过exchange交换数据，若第一个线程先执行exchange，会一直等待第二个线程也执行exchange，当都到达同步点时，
两个线程可以交换数据。

应用场景：
+ 遗传算法，需要选两个人作为交配对象，这时交换两个人数据，并用交叉规则得出2个交配结果。
+ 用于校对工作。为避免错误，采用AB岗两人进行录入，对两个Excel数据进行校验。
参见：ExchangerTest
肉有一个线程没执行exchange则会一直等待，若担心有特殊情况发生，避免一直等待，可以用exchange(V x, long timeout,
TimeUnit unit)设定最大等待时长

### 第9章，java中的线程池
合理地使用线程池能带来好处：
+ 降低资源消耗。
重复利用已创建的线程降低线程创建和销毁造成的消耗
+ 提高响应速度
当任务到达时，任何可以不用等到线程创建就能立即执行
+ 提高线程的可管理型
线程时稀缺资源，若无限之地创建，不仅会消耗系统资源，还会降低系统的稳定性。
使用线程池可以统一分配、调优和监控

要做到合理利用线程池，必须对其实现原理了如指掌

### 线程池的实现原理
当提交一个新任务到线程池时，线程池的处理流程：
+ 判断核心线程池里的线程是否已满。
  + 若不是，则创建一个新的工作线程来执行任务
	+ 若都在执行任务，则进入下个流程
+ 线程池判断工作队列是否已经满。
  + 若没满，将新提交的任务存储在里面
	+ 若满了，进入下个流程
+ 线程池是否已满
  + 若没有，则创建一个新的工作线程执行任务
	+ 若满了，交给饱和策略来处理这个任务

ThreadPoolExecutor执行execute分为4种情况：
1)若当前运行的线程少于corePoolSize，则创建新线程执行任务(执行这一步要获取全局锁)
2)若运行的线程等于或多于corePoolSize，则将任务加入BlockingQueue
3)若无法将任务加入到BlockingQueue(队列已满)，创建新的线程处理任务(这一步需要获取全局锁)
4)若创建新线程将使当前运行的线程超出maximumPoolSize，任务将被拒绝，
调用RejectedExecutionHandler.rejectedExecution方法

采用上述步骤的总体设计思路，是为了在执行execute时，尽可能避免获取全局锁(那将会是一个严重的可伸缩瓶颈)。
在完成预热后(当前运行的线程数大于等于corePoolSize)，几乎所有的execute调用都是执行步骤2，不需要获取全局锁。

```todo 这部分注释和代码有些不匹配
public void execute(Runnable command){
  if(command == null){
	  throw new NullPointerException();
	}
	// 若线程数小于基本线程数，则创建线程并执行当前任务
	if(poolSize >= corePoolSize || !addIfUnderCorePoolSize(command)){}
	// 若线程数大于等于基本线程数或线程创建失败，则将当前任务放到工作队列中
  if(runState == RUNNING && workQueue.offer(command)){
	  if(runState != RUNNING || poolSize == 0){
		  ensureQueuedTaskHandled(command);
		}
	}
	// 若线程池不处于运行中或任务无法放入队列，并且当前线程数小于最大允许的线程数，则创建一个线程执行任务
	else if(!addIfUnderMaximumPoolSize(command)){}
	// 抛出RejectedExecutionException异常
	reject(command);  // is shutdown or saturated
	}
}
```

线程池创建线程时，会将线程封装成工作线程Worker，Worker在执行完任务后，会循环获取工作队列里的任务来执行
```
public void run(){
  try{
	  Runnable task = firstTask;
		firstTask = null;
		while(task != null || (task = getTask()) != null){
		  runTask(task);
			task = null;
		}
	}finally{
	  workerDone(this);
	}
}
```

1. 线程池的创建
new ThreadPoolExecutor(corePoolSize, maximumPoolSize, keepAliveTime, milliseconds, runnableTaskQueue,
handler);
几个参数：
+ corePoolSize：当提交一个任务到线程池时，线程池会创建一个线程来执行任务，即使其他空闲的基本线程能执行新任务也
会创建线程，等到需要执行的任务数大于线程池基本大小时不再创建。
若调用了线程池的prestartAllCoreThreads，线程池会提前创建并启动所有基本线程
+ runnableTaskQueue
用于保存等待执行的任务的阻塞队列。可选
  + ArrayBlockingQueue：基于数组结构的有界队列，FIFO对元素排序
	+ LinkedBlockingQueue：基于链表结构的阻塞队列，按FIFO，吞吐量通常高于ArrayBlockingQueue。
	Executors.newFixedThreadPool用这个队列。
	+ SynchronousQueue：不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直阻塞，
	吞吐量高于LinkedBlockingQueue。Executos.newCachedThreadPool用这个队列
	+ PriorityBlockingQueue：具有优先级的无限阻塞队列
+ maximumPoolSize
线程池允许创建的最大线程数。若队列满，且已创建的线程数小于最大线程数，则线程池会在创建新的线程执行任务。若用了无界
的任务队列，则这个参数无效果
+ ThreadFactory
用于创建线程的工厂，可给每个创建出来的线程命名。guava用的ThreadFactoryBuilder可快速给线程池的线程设置名字。
new ThreadFactoryBuilder().setNameFormat("XX-task-%d").build();
+ RejectedExecutionHandler
当线程和任务池都满了，说明线程池处于饱和状态，必须采用一种策略处理提交的新任务。
jdk5提供4种策略：
  + AbortPolicy：默认，直接抛出异常
  + CallerRunsPolicy：用调用者所在线程来运行任务
  + DiscardOldestPolicy：丢弃队列里最近的一个任务，并执行当前任务
  + DiscardPolicy：不处理，丢弃掉
也可自定义，如记录日志或持久化存储不能处理的任务
+ keepAliveTime
线程池的工作线程空闲后，保持存活的时间。
若任务很多，每个任务执行的时间比较短，可调大时间，提高线程的利用率
+ TimeUnit
线程活动保持时间的单位。DAYS/HOURS/MINUTES/MMILLISECONDS/MICROSECONDS/NANOSECONDS

2. 向线程池提交任务
两个方法
+ execute用于提交不需要返回值的任务。
+ submit用于提交需要返回值的任务
```java
Future<Object> future = executor.submit(harReturnValuetask);
try{
  Object s = future.get();
}catch(InterruptedException e){
  // 处理中断异常
}catch(ExecutionException e){
  // 处理无法执行任务异常
  ExecutionException – if the computation threw an exception
  应该是业务抛出的异常
}finally{
  // 关闭线程池
}
```

3. 关闭线程池
shutdown或shutdownNow关闭线程池。
原理是遍历线程池中的工作线程，逐个调用线程的interrupt方法中断线程，所以无法响应中断的任务可能永远无法终止。
区别：
+ shutdownNow首先将线程池的状态设置成STOP，然后尝试停止所有的正在执行或暂停任务的线程，并返回等待执行任务的
列表
+ shutdown只是将线程池的状态设置成SHUTDOWN状态，然后中断所有没有正在执行任务的线程。

调用上述任一一个，isShutdown返回true。
当所有任务都已关闭后，才表示线程池关闭成功，这时isTerminated会返回true。

由提交到线程池的任务特性决定，通常用shutdown，若任务不一定要执行完，可以调用shutdownNow

4. 合理地配置线程池
先分析任务特性，从几个角度：
+ 任务的性质：CPU密集型任务、IO密集型任务和混合型任务
+ 任务的优先级：高、中和低
+ 任务的执行时间：长、中和短
+ 任务的依赖性：是否依赖其他系统资源，如数据库连接

性质不同的任务可以用不同规模的线程池分开处理。
CPU密集型任务应配置尽可能小的线程，如Ncpu+1。
由于IO密集型任务并不是一直在执行任务，则应配置尽可能多的线程，如2*Ncpu。
混合型的任务，若可以拆分，将其拆分成一个CPU密集型任务和一个IO密集型任务，
  + 只要这两任务执行的时间相差不是太大，那么分解后执行的吞吐量将高于串行执行的吞吐量。
	+ 若相差太大，则没有必要进行分解。

优先级不同的任务可以用PriorityBlockingQueue来处理。
注意：若一直有优先级高的任务提交到队列里，那么优先级低的任务可能永远不能执行

执行时间不同的任务可以交给不同规模的线程池处理，或可以用优先级队列，让执行时间短的任务先执行

依赖数据库连接池的任务，因为线程提交SQL后需要等待数据库返回结果，等待的时间越长，则CPU空闲时间越长，那么线程数
应该设置得越大，才能更好地利用CPU

建议用有界队列。能增加系统的稳定性和预警能力，可根据需要设大一点，比如几千。

5. 线程池的监控
通过线程池的参数进行监控：
+ taskCount：线程池需要执行的任务数量
+ completedTaskCount：线程池在运行过程中已完成的任务数量，小于或等于taskCount
+ largestPoolSize：线程池曾经创建过的最大线程数量。可知道线程池是否曾经满过。若等于线程池的最大大小，则表示线程池
曾经满过。
+ getPoolSize：线程池的线程数量。若线程池不销毁的话，线程池里的线程不会自动销毁，所以这个大小只增不减
+ getActiveCount：获取活动的线程数

通过扩展线程池进行监控：
可通过继承线程池来自定义线程池，重写beforeExecute、afterExecute和terminated方法，也可在任务执行前、执行后和
线程池关闭前执行一些代码来进行监控。
例如，监控任务的平均执行时间、最大执行时间和最小执行时间等。


## 第10章，Executor框架
java的线程即是工作单元，也是执行机制。
jdk5把工作单元和执行机制分离。工作单元包括Runnable和Callable，而执行机制由Executor框架提供

### Executor框架简介
1. Executor框架的两级调度模型
在HotSpot vm的线程模型中，java线程(Thread)被一对一映射为本地操作系统线程。
java线程启动时会创建一个本地操作系统线程；当该java线程终止时，这个操作系统线程也会被回收。
操作系统会调度所有线程并将他们分配给可用的CPU

两级调度模型：
上层，java多线程程序通过把应用分解成若干个任务，使用用户级的调度器(Executor框架)将这些任务映射为固定数量的线程
在底层，操作系统内核将这些线程映射到硬件处理器上。

2. Executor框架的结构和成员
1)Executor框架的结构
3部分组成：
+ 任务。
包括被执行任务需要实现的接口：Runnable或Callable
+ 任务的执行
包括任务执行机制的核心接口Executor，及继承自Executor的ExecutorService接口。
两个实现ExecutorService接口的类ThreadPoolExecutor和ScheduledThreadPoolExecutor
+ 异步计算的结果。
包括接口Future和实现Future的FutureTask类

类和接口简介：
+ Executor接口，将任务的提交与任务的执行分离
+ ThreadPoolExecutor是核心实现类，用来执行被提交的任务
+ ScheduledThreadPoolExecutor是一个实现类，在给定的延迟后运行命令，或定期执行命令。比Timer更灵活强大
+ Future接口和实现类FutureTask，代表异步计算的结果
+ Runnable接口和Callable接口的实现类，可被ThreadPoolExecutor或ScheduledThreadPoolExecutor执行

将Runnable对象封装成为一个Callable对象
Executors.callable(Runnable task)/Executors.callable(Runnable task, Object result)

2)Executor框架的成员
a.ThreadPoolExecutor
通常用Executors创建。3种：
+ FixedThreadPool。固定线程数
适用于为了满足资源管理的需求，而需要限制当前线程数量的应用场景，适用于负载比较重的服务器
+ SingleThreadExecutor
单个线程
适用于需要保证顺序地执行各个任务；并在任意时间点，不会有多个线程是活动的应用场景
+ CachedThreadPool
根据需要创建新线程
是大小无界的线程池，适用于执行很多的短期异步任务的小程序，或是负载较轻的服务器

b.ScheduledThreadPoolExecutor
通常用Executors创建，2种：
+ ScheduledThreadPoolExecutor。
包含若干个线程
适用于需要多个后台线程执行周期任务，同时为满足资源管理的需求而需要限制后台线程的数量的应用场景
+ SingleThreadScheduledExecutor。
只包含一个线程。
适用于需要单个后台线程执行周期任务，同时需要保证顺序地执行各个任务的应用场景

c.Future接口
Future和FutureTask用来表示异步计算的结果。

d.Runnable接口和Callable接口
其实现类可被ThreadPoolExecutor或ScheduledThreadPoolExecutor执行。
Runnable不会返回结果，Callable可以返回结果

### ThreadPoolExecutor详解
是线程池的实现类，主要4个组件构成：
+ corePool：核心线程池的大小
+ maximumPool：最大线程池的大小
+ BlockingQueue：用来暂时保存任务的工作队列
+ RejectedExecutionHandler：当ThreadPoolExecutor已经关闭或ThreadPoolExecutor已经饱和时(达到最大线程池
大小且工作队列已满)，execute将要调用的Handler

1. FixedThreadPool详解
可重用固定线程数的线程池。
corePoolSize和maximumPoolSize都设为nThreads  -- 有任务来一直创建直到maximumPoolSize
当线程池中的线程数大于corePoolSize时，keepAliveTime为多余的空闲线程等待新任务的最长时间，超过后则多余的线程将
被终止。这里设置为0L，表示多余的空闲线程会被立即终止。  -- 不过core和max都一样，也就没有多余的了

1)若当前运行的线程数少于corePoolSize，则创建新线程来执行任务
2)在线程池完成预热后(当前运行的线程数等于corePoolSize)，将任务加入LinkedBlockingQueue
3)线程执行完1中的任务后，会在循环中反复从LinkedBlockingQueue获取任务执行

FixedThreadPool用无界队列LinkedBlockingQueue作为工作队列(容量为Integer.MAX_VALUE)。
用无界队列对线程池带来影响：
1)当线程池中的线程数达到corePoolSize后，新任务将在无界队列中等待，因此线程中的线程数不会超过corePoolSize
2)由于1，maximumPoolSize将是一个无效参数
3)由于1和2，keepAliveTime将是一个无效参数
4)由于用无界队列，运行中的FixedThreadPool(未执行shutdown或shutdownNow)不会拒绝任务(
不会调用RejectedExecutionHandler.rejectedExecution)

2. SingleThreadExecutor详解
使用单个worker线程的Executor
corePoolSize和maximumPoolSize都是1，keepAliveTime=0，用LinkedBlockingQueue，也是无界

说明：
1)若当前运行的线程数少于corePoolSize(即线程池中午运行的线程)，则创建一个新线程来执行
2)在线程池完成预热后(当前线程池中有一个运行的线程)，将任务加入LinkedBlockingQueue
3)线程执行完1中的任务后，会在一个无限循环中反复从LinkedBlockingQueue获取任务执行

3. CachedThreadPool详解
会根据需要创建新线程。
corePoolSize=0，maximumPoolSize=Integer.MAX_VALUE。keepAliveTime=60L，空闲线程等待新任务的最长时间为60s，
超过后将会被终止。使用没有容量的SynchronousQueue队列。若主线程提交任务的速度高于maximumPoolSize中线程处理任务
的速度时，CachedThreadPool会不断创建新线程(因为SynchronousQueue需要两端都有才能使得execute时不阻塞，进而不用
创建线程)。
极端时会创建过多线程而耗尽CPU和内存资源

说明：
1)执行SynchronousQueue.offer(task)。
若当前池中有空闲线程正在执行SynchronousQueue.poll(keepAliveTime, TimeUnit)，那么主线程执行offer操作与空闲
线程执行pool操作配对成功，主线程把任务交给空闲线程执行，否则执行下面
2)创建一个新线程执行任务
3)在2中新创建的线程执行任务完后，会执行poll，会让空闲线程最多在SynchronousQueue中等待60s，若60s内主线程提交了
一个新任务，那么它将执行主线程提交的任务；否则，空闲线程将终止。
由于空闲60s将会被终止，因此长时间保持空闲的CachedThreadPool不会使用任何资源

### ScheduledThreadPoolExecutor详解
继承ThreadPoolExecutor，主要用来在给定的延迟后运行任务，或定期执行任务。

1. ScheduledThreadPoolExecutor的运行机制
DelayQueue是一个无界队列，所以maximumPoolSize在ScheduledThreadPoolExecutor中没有意义。

执行主要分为两大部分：
1)当调用scheduleAtFixedRate或scheduleWithFixedDelay时，会向DelayQueue添加一个实现了RunnableScheduledFuture
接口的ScheduledFutureTask
2)线程池中的线程从DelayQueue中获取ScheduledFutureTask，然后执行任务

ScheduledThreadPoolExecutor为实现周期性执行任务，对ThreadPoolExecutor做了修改：
+ 用DelayQueue
+ 获取任务的方式不同
+ 执行周期任务后，增加了额外的处理

2. ScheduledThreadPoolExecutor实现

ScheduledFutureTask主要包含3个成员变量：
+ long型time，表示这个任务将要被执行的具体时间
+ long型sequenceNumber，表示这个任务呗添加到ScheduledThreadPoolExecutor中的序号
+ long型period，表示任务执行的间隔周期

DelayQueue封装了一个PriorityQueue，可对队列中的ScheduledFutureTask排序。time小的排在前面。
若time相同，则比较sequenceNumber小的在前。

一个线程执行某个周期任务的步骤：
1)从DelayQueue中获取已到期的ScheduledFutureTask(take)，到期任务指time大于等于当前时间
2)执行这个ScheduledFutureTask
3)修改ScheduledFutureTask的time变量为下次将要被执行的时间
4)把这个修改后的ScheduledFutureTask放回DelayQueue(add)

步骤1)过程源码take
```java
public E take(){
  final ReentrantLock lock = this.lock;
	lock.lockInterruptibly();
	try{
	  for(;;){  // 只有线程从队列中获取到元素后才会退出
		  E first = q.peek();
			if(first == null){
			  available.await();  // 为空，当前线程在condition中等待
			}else{
			  long delay = first.getDelay(TimeUnit.NANOSECONDS);
				if(delay > 0){  // time比当前时间大，到condition中的等待time时间
				  long t1 = available.awaitNanos(delay);
				}else{
				  E x = q.poll();
					assert x != null;
					if(q.size != 0){  // 队列不为空，唤醒在condition上等待的所有线程，让多线程进行消费
					  available.signalAll();
					}
					return x;
				}
			}
		}
	}finally{
	  lock.unlock();
	}
}
```

```
public bollean offer(E e){
  final ReentrantLock lock = this.lock;
	lock.lock();
	try{
	  E first = q.peek();
		q.offer(e);
		// 若当前添加的是头元素，或time都比first还小，则唤醒所有在condition中等待的线程
		if(first == null || e.compareTo(first)){
		  available.signalAll();
		}
		return true;
	}finally{
	  lock.unlock();
	}
}
```

### FutureTask详解
Future接口和实现类FutureTask，代表异步计算的结果

1. FutureTask简介
FutureTask还实现了Runnable接口。

FutureTask可处于3中状态：
+ 未启动。
刚创建，并没有执行run方法前
+ 已启动。
run被执行的过程中。
+ 已完成
run方法执行完成后正常结束，或被取消(cancel)或执行时抛出异常而异常结束

调用get:
+ 当FutureTask处于未启动或已启动时，导致调用线程阻塞
+ 当处于已完成时，导致调用线程立即返回结果或抛出异常

调用cancel:
+ 当处于未启动状态时，导致此任务永远不会被执行
+ 当处于已启动时
  + 调用cancel(true)将以中断执行此任务线程的方式试图停止任务；
  + 调用cancel(false)将不会对正在执行此任务的线程产生影响(让其运行完成，不中断)；
+ 当处于完成状态时，调用cancel将返回false

当一个线程需要等待另一个线程把某个任务执行完后才能继续执行，可以用FutureTask
当多个线程同时执行一个任务时，只允许一个线程执行，其他线程需要等待这个任务的直接结果。
```java
private finla ConcurrentMap<Object, Future<String>> taskCache = new ConcurrentHashMap<>();

private String executionTask(final String taskName){
  while(true){
	  Future<String> future = taskCache.get(taskName);
		if(future == null){
		  Callable<String> task = new Callable<>(){
			  public String call(){
				  return taskName;
				}
			};
			FutureTask<String> futureTask = new FutureTask<>(task);
			future = taskCache.putIfAbsent(taskName, futureTask);
			if(future == null){  // 只有第一次放入成功的才能执行任务
			  future = futureTask;  // 赋值，让下面get可以让自己等待，其他线程由于不为空直接在下面等待
				futureTask.run();
			}
		}
		try{
		  return future.get();  // 所有线程在此等待任务执行完成
		}catch(CancellationException e){
		  taskCache.remove(taskName, future);
		}
	}
}
```

3. FutureTask的实现
基于AQS。

每一个基于AQS实现的同步器都会包含两种类型的操作：
+ 至少一个acquire操作。
此操作阻塞调用线程，除非/直到AQS的状态允许这个线程继续执行。
FutureTask的acquire操作为get/get(timeout)
+ 至少一个release操作
这个操作改变AQS的状态，改变后可允许一个或多个阻塞线程被解除。
FutureTask的release操作包括run和cancel

AQS被作为"模板方法模式"的基础类提供给FutureTask的内部子类Sync。
Sync实现了AQS的tryAcquireShared和tryReleaseShared方法，通过俩方法检查和更新同步状态

创建FutureTask时会创建内部私有的成员对象Sync，FutureTask所有共有方法都直接委托给Sync

get会调用AQS.acquireSharedInterruptibly，过程:
+ 先会回到子类的tryAcquireShared判断acquire操作是否可以成功。
可以成功的条件为：state为执行完成状态RAN或已取消状态CANCELLED，且runner不为null
+ 若成功则get立即返回。若失败则到线程等待队列中去等待其他线程执行release操作
+ 其他线程执行release操作(如run或cancel)唤醒当前线程后，当前线程再次执行tryAcquireShared将返回1，当前线程
离开线程等待队列并唤醒它的后继线程(会产生级联唤醒)
+ 最后返回计算的结果或抛出异常

FutureTask.run过程：
+ 执行在构造函数中指定的任务(Callable.call())
+ 以原子方式更新同步状态(用AQS.compareAndSetState设置state为执行完成RAN)。若cas成功，就设置代表结算结果的变量
result的值为Callable.call()的返回值，然用后AQS.releaseShared
+ AQS.releaseShared会回调子类中tryReleaseShared执行release操作(设置运行任务的线程runner为null，然后返回true)；
AQS.releaseShared，然后唤醒线程等待队列中的第一个线程
+ 调用FutureTask.done

当执行get时，若FutureTask不是处于执行完成状态RAN或已取消状态CANCELLED，当前线程将到AQS的线程等待队列中等待。
当某个线程执行FutureTask.run或cancel时，会唤醒线层等待队列中的第一个线程。
被唤醒的线程，先把自己从队列中删除，然后唤醒它的后继线程，最后从get返回。后续线程重复此流程。


## 第11章，java并发编程实战

### 生产者和消费者模式
通过平衡生产者和消费者线程的工作能力来提高程序整体处理数据的速度
为了解决这汇总生产消费能力不均衡的问题，便有了生产者和消费者模式
通过一个容器来解决生产者和消费者的强耦合问题。彼此之间不直接通信，而是通过阻塞队列进行通信，所以生产者生产完
数据后不用等待消费者处理，直接扔给阻塞队列，消费者不找生产者要数据，而是直接从队列里取，阻塞队列相当于一个
缓冲区，平衡了两者的处理能力
多数设计模式，都会找一个第三者来进行解耦，如工厂模式的第三者是工厂类，模板模式的第三者是模板类。

1. 生产者和消费者模式实战
Yuna工具使用生产者和消费者模式
解决通过赋值-黏贴-发送邮件的分享，不能使技术文章沉淀和找到之前分享文章问题。
专门申请一个用来收集分享邮件的邮箱share@alibaba.com，加入到部门邮件列表，大家将文章按之前方式发送到部门。

Yuna通过读取邮件服务器里该邮箱的邮件，把素有分享的邮件下载，要求分享的邮件标题带有关键字，如"内贸技术分享"。
通过confluence的Web Service接口，把文章插入confluence里。
还可以用Yuna自动把文章分类和归档

1.0版单线程，因为当时只需要处理一个部门的邮件，过程串行。
程序先抽取全部邮件，转化为文章对象，然后添加全部的文章，最后删除抽取过的邮件。
```
public void extract(){
  logger.debug("开始"+getExtractorName()+"..");
	// 抽取邮件
	List<Article> articles = extractEmail();
	// 添加文章
	for(Article article : articles){
	  addArticleOrComment(article);
	}
	// 清空邮件
	cleanEmail();
	logger.debug("完成"+getExtractorName()+"..");
}
```

推广后，很多部门用，处理时间雨来越慢，每隔5分钟进行一次抽取，而当邮件太多，一次处理可能花几分钟，于是2.0版用了生产
者消费者模式处理邮件，生产者按一定的规则去邮件系统抽取邮件，存放到阻塞队列，消费者从阻塞队列取文章后插入到
conflunce里
```
public class QuickEmailToWikiExtractor extends AbstractExtractor{
  private ThreadPoolExecutor threadsPool;
	private ArticlesBlockingQueue<ExchangeEmailShallowDTO> emailQueue;

	public QuickEmailToWikiExtractor(){
	  emailQueue = new ArticleBlockingQueue<ExchangeEmailShallowDTO>();
		int corePoolSize = Runtime.getRuntime().availableProcessors() * 2;
		threadsPool = new ThreadPoolExecutor(corePoolSize, corePoolSize, 10l, TimeUnit,SECONDS,new
		LinkedBlockingQueue<Runnable>(2000));
	}

	public void extract(){
	  logger.debug("开始"+getExtractorName()+"..");
		long start = System.currentTimeMillis();

		// 生产者-抽取所有邮件放到队列，单线程
		new ExtractEmailTask().start();

		// 消费者-把队列里的文章插入到Wiki，多线程
		insertToWiki();

		long end = System.currentTimeMillis();
		double cost = (end - start) / 1000;
		logger.debug("完成"+getExtractorName()+"，花费时间:"+cost+"s");
	}

	// 把队列的文章插入到Wiki
	private void insertToWiki(){
	  // 登录Wiki，每间隔一段时间需要登陆一次
		confluenceService.login(RuleFactory.USER_NAME, RuleFactory.PASSWORD);

		while(true){
		  // 2秒内取不到就退出
			ExchangeEmailShallowDTO email = emailQueue.poll(2, TimeUnit.SECONDS);
			if(email == null){
			  break;
			}
			threadsPool.submit(new insertToWikiTask(email));
		}
	}

	protected List<Article> extractEmail(){
	  List<ExchagneEmailShallowDTO> allEmails = getEmailService().queryAllEmails();
		if(allEmails == null){
		  return null;
		}
		for(ExchagneEmailShallowDTO exchagneEmailShallowDTO : allEmails){
		  emailQueue.offer(exchagneEmailShallowDTO);
		}
		return null;
	}

	// 抽取邮件任务
	public class ExtractEmailTask extends Thread{
	  public void run(){
		  extractEmail();
		}
	}
}
```

2. 多生产者和多消费者场景
在一个长连接服务器中用了这种模式，生产者1负责将所有客户发来的消息存放在阻塞队列1里，消费者1从队列读消息，
然后通过消息ID进行散列得到N个队列中的一个，根据编号将消息存放到不同队列里，每个阻塞队列会分配一个线程来消费
阻塞队列中数据。若消费者2无法消费信息，就将消息抛回到阻塞队列1中，交给其他消费者处理
```java
// 消息队列管理
public class MsgQueueManager implements IMsgQueue{
  logger
	// 消息总队列
	public final BlockingQueue<Message> messageQueue;
	private MsgQueueManager(){
	  messageQueue = new LinkedTransferQueue<Message>();
	}

	public void put(Message msg){
	  try{
		  messageQueue.put(msg);
		}catch(InterruptedException e){
		  Thread.currentThread().interrupt();
		}
	}

	public Message take(){
	  try{
		  return messageQueue.tale();
		}catch(InterruptedException e){
		  Thread.currentThread().interrupt();
		}
		return null;
	}
}

启动一个消息分发线程，这个线程里子队列自动去总队列里获取消息
// 分发消息，负责把消息从队列转放到小队列，调度
static class DispatchMessageTask implements Runnable{
  public void run(){
	  BlockingQueue<Message> subQueue;
		for(;;){
		  // 若没数据，阻塞
			Message msg = MsgQueueFactory.getMessageQueue().take();
			// 若为空，表示没有Session机器连接上来，需要等待，直到有Session机器连接上来
			while((subQueue = getInstance().getSubQueue()) == null){
			  try{
				  Thread.sleep(1000);
				}catch(InterruptedException e){
		      Thread.currentThread().interrupt();
		    }
			}
			// 把消息放入小队列
			try{
			  subQueue.put(msg);
			}catch(InterruptedException e){
		    Thread.currentThread().interrupt();
		  }
		}
	}
}

用散列算法获取一个子队列
// 均衡获取一个子队列
public BlockingQueue<Message> getSubQueue(){
  int errorCount = 0;
	for(;;){
	  if(subMsgQueues.isEmpty()){
		  return null;
		}
		in index = (int)(System.nanoTime()%subMsgQueues.size());
		try{
		  return subMsgQueues.get(index);
		}catch(Exception e){
		  // 出错误，表示：在获取队列大小后size，队列进行了一次删除操作，所以get的index不存在了
			logger.error("获取子队列出错", e);
			if((++errorCount < 3)){
			  continue;
			}
		}
	}
}

使用时，只需往总队列里发消息
IMsgQueue messageQueue = MsgQueueFactory.getMessageQueue();
Packet msg = Packet.createPacket(Packet64FrameType.TYPE_DATA, "{}".getBytes(), (short)1);
messageQueue.put(msg);
```

3. 线程池与生产消费者模式
java的线程池就是生产者消费者模式，不过更高明。生产者把任务丢给线程池，线程池创建线程并处理任务，若将要运行的任务
数大于线程池的基本线程数，就把任务放入阻塞队列，比只用一个阻塞队列来实现生产者和消费者模式高明，因为消费者能够处理
直接就处理掉了，速度更快，而生产者先存，消费者再取这种方式显然慢一些

我们系统也可以用线程池实现多生产者和消费者模式。例如，创建N个不同规模的线程池处理不同性质的任务，线程池1将数据读
到内存后，交给线程池2的线程处理压缩数据。线程池1主要处理IO密集型任务，线程池2主要处理CPU密集型任务

应用场景很多，如上传附件并处理，用户上传，系统把文件放入队列，立刻返回客户，最后消费者再去队列取文件处理。
再如，调用一个远程接口查询数据，若远程服务接口查询时需要几十秒时间，那么它可以提供一个申请查询的接口，把要申请查
询任务放数据库中，然后接口立刻返回，然后服务器端用线程轮询并获取申请任务进行处理，处理后发消息给调用方，让调用方
再来调用另一个接口取数据

### 线上问题定位
1)linux中top命令，查看每个进程的情况
我们程序是java应用，只需关注COMMAND是java的性能数据，可看到java进程这一行CPU利用率300%，这个是当前机器所有核
加载一起的CPU利用率
2)再用top的交互命令数字1查看每个CPU的性能数据
显示了CPU4，说明这是一个5核的虚拟机，平均每个CPU利用率在60%以上。
若这里CPU利用率100%则可能程序有了死循环

CPU参数含义：
参数 描述
us  用户空间占用CPU百分百比
1.0%sy  内核空间占用CPU百分比
0.0%ni  用户进程空间内改变过优先级的进程占用CPU百分比
98.7%id  空闲CPU百分比
0.0% wa  等待输入/输出的CPU时间百分比

3)使用top的交互命令H查看每个线程的性能
三种可能情况：
a. 某个线程CPU利用率一直100%，说明这个线程可能死循环，记住这个PID
b. 某个线程一直在TOP10，说明这个线程可能有性能问题
c. CPU利用率搞得几个线程在不停变化，说明并不是由某一个线程导致CPU偏高

若a，也可能是GC造成，可以用jstat看下GC情况，是不是由于持久代或老年代满了，产生Full GC，导致CPU利用率持续高。
jstat -gcutil pid 1000 5
可以把线程dump下来，看看究竟是哪个线程、执行什么代码造成的CPU利用率高
jstack pid > dump11
dump出来的线程ID(nid)是十六进制的，而用TOP看到的线程ID时十进制，要用printf转换，
printf "%x\n" 31558
然后用输出的十六进制的ID去dump文件里找线程

### 性能测试
希望系统的某个接口能支持2W的QPS，而我们应用部署在多台机器上，要支持2W的QPS，必须先知道该接口在单机上支持多少QPS，
若单机能1k，那么需要20台机器。注意，2W的QPS必须是峰值，而不能是平均值。

先性能测试。工具原理是，用户写一个java程序向服务器端发起请求，这个工具会启动一个线程池来调度这些任务，可以配置同时
启动多少个线程、发起请求次数和任务间隔时长。
将这个程序部署在多台机器上执行，统计出QPS和响应时长。
在10台机器上部署测试程序，每台机器启动了100个线程进行测试，压测时长30m，注意不能压测线上机器，这里是开发服务器

测试开始后，登录服务器看当前有多少台机器在压测服务器，程序端口时12200，用netstat查询多少台机器连接到这个端口
netstat -nat | grep 12200 -c

当QPS达到1400，程序开始报错获取不到数据库连接，用netstat看已经使用了多少个数据库连接
netstat -nat | grep 3306 -c

增加数据库连接到20，QPS没上去，但响应时长从平均1000ms下降到700ms，用TOP观察CPU利用率，发现已经90%多了，
-- 用于操作db的线程数多了，就可以并发执行sql了，然后对于本次请求的响应时长中，就可以不用再阻塞之前的有限线程了
于是升级CPU，将2核升级成4核，和线上的机器保持一致。
在压测，CPU利用率下去了，达到75%，QPS上升到1800，执行一段时间后响应时长稳定在200ms

增加应用服务器里线程池的核心线程数和最大线程数到1024，通过ps看线程数是否增长了
ps -eLf | grep java -c

再次压测，QPS没有明显增长，单机QPS稳定在1800左右，响应时长稳定在200ms
--请求并发没有变，而线程数变多了，而QPS没上去，是不是还是卡在了数据库的20个连接上。业务线程都等着从db连接中获取数据，或阻塞在获取其线程上

性能测试之前先优化程序的SQL，用如下命令统计执行最慢的SQL
grep Y /xxxmonitor.log | awk -F ',' '{print $7$5}' | sort -nr |head -20

性能测试中其他命令：
1)查看网络流量
cat /proc/net/dev

2)查看系统平均负载
cat /proc/loadavg

3)查看系统内存情况
cat /proc/meminfo

4)查看CPU利用率
cat /proc/stat

### 异步任务池
某些场景需要对线程池进行扩展才能更好的服务于系统。
例如，若一个任务扔进线程池后，运行线程池的程序重启了，那么线程池的任务就会丢失。
另外，线程池只能处理本机的任务，在集群环境下不能有效地调度所有机器的任务。

需要集合线程池开发一个异步任务处理池

任务池的主要流程：每台机器都会启动一个任务池，每个任务池里有多个线程池，当某台机器将一个任务交给任务池后，任务池会
先将这个任务保存到数据库，然后某台机器上的任务池会从数据库中获取待执行的任务，再执行这个任务
--预写日志
每个任务有几种状态：
+ 创建(NEW)
提交给任务池后的状态
+ 执行中(EXECUTING)
任务池从数据库中拿到任务执行时的状态
+ RETRY(重试)
当执行任务时出错，程序显示地告诉任务池这个任务需要重试，并设置下一次执行时间
+ 挂起(SUSPEND)
当一个任务的执行依赖于其他任务完成时，可以将这个任务挂起，当收到消息后，再开始执行
+ 中止(TEMINER)
任务执行失败，让任务池停止执行这个任务，并设置错误消息告诉调用端
+ 执行完成(FINISH)
任务执行结束

任务池的任务隔离
异步任务有很多类型，不同类型的任务优先级不一样，但系统资源有限，必须对任务进行隔离执行。用不同的线程池处理不同的
任务，或不同的线程池处理不同的优先级任务，若任务类型非常少，建议用任务类型来隔离，若任务类型非常多，如十几个，
建议采用优先级的方式隔离

任务池的重试策略。
根据不同的任务类型设置不同的重试策略，有的任务对实时性要求高，那么每次重试间隔就会非常短，否则可以采用默认的
重试策略，间隔随次数的增加时间不断增加，比如间隔几秒、几分钟到几小时。
每个任务类型可以设置执行该任务类型线程池的最大和最小线程数、最大重试次数

使用任务池的注意事项
任务必须无状态。数据放在集群中

异步任务的属性
包括任务名称、下次执行时间、已执行次数、任务类型、任务优先级和执行时的报错信息(用于快速定位问题)
